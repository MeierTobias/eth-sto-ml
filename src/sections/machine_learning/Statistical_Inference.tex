\section{Statistical Inference}
Statistical learning can be split into
\begin{itemize}
    \item \textbf{Frequentism}:
          \begin{enumerate}
              \item Pick a parametric model
              \item Fit the model using MLE
          \end{enumerate}
          \newpar{}
          \begin{itemize}
              \item[+] Tractable
              \item[+] Asymptotically unbiased
              \item[-] Stability and variance issues
          \end{itemize}
          \newpar{}
    \item \textbf{Bayesianism}:
          \begin{enumerate}
              \item Guess a prior model of the parameter
              \item Pick a parametric model
              \item Update the prior with the parametric model to obtain a posterior model
          \end{enumerate}
          \newpar{}
          \begin{itemize}
              \item[+] Low variance
              \item[-] Intractable
              \item[-] Bias issues
          \end{itemize}
\end{itemize}

\subsection{Frequentism}
By choosing a model $\mathcal{H}$, one can introduce prior knowledge.

\subsubsection{Parametric Model}
Assuming $X\sim F$ where $F$ is any distribution, the goal is to find parameters $\theta\in\Theta$ such that the model $\mathcal{H}$ ``best fits'' the samples $X$.

\newpar{}
\ptitle{Example}
\noindent\begin{equation*}
    \mathcal{H}=\Bigl\{\mathcal{N}(\theta, \sigma^2): \theta\in\Theta\Bigr\}\qquad \begin{cases}
        \Theta = [40,100] \\
        \sigma^2 = 10
    \end{cases}
\end{equation*}
or in the multivariate case
\noindent\begin{equation*}
    \mathcal{H}=\Bigl\{\mathcal{N}(\mu, \Sigma): \mu\in\Theta_\mu, \Sigma\in\Theta_\Sigma\Bigr\}\quad \begin{cases}
        \Theta_\mu\subseteq \mathbb{R}^{d+1} \\
        \Theta_\Sigma\subseteq \mathbb{R}^{(d+1)\times (d+1)}
    \end{cases}
\end{equation*}

\subsubsection{MLE}
The \textit{maximum likelyhood estimator} $\theta^*$ is given by
\noindent\begin{equation*}
    \theta^* = \arg\max_{\theta\in\Theta} \log(p(X|\theta)) \overset{X\sim\mathcal{N}}{=} \frac{1}{n} \sum_{i=1}^{n} x_i
\end{equation*}

\textbf{Remark}

By the law of large numbers if $X\sim \mathcal{N}(\theta_0, \sigma^2)$, $\theta^*\to\theta_0$ as $n\to\infty$

\newpar{}
\ptitle{Alternative Model}

If the model
\noindent\begin{equation*}
    y_i\sim \mathcal{N}({\beta_0}^{\mathsf{T}}x_i, \sigma^2)\quad \sigma>0, \beta_0\in \mathbb{R}^d, i\leq n
\end{equation*}
is chosen, the \textbf{MLE} is equal to the \textbf{OLSE}:
\noindent\begin{align*}
    \log(p(X|\beta))&= C -\frac{1}{{\sigma_0}^2}\sum_{i=1}^{n} {(y_i-\beta^{\mathsf{T}}x_i)}^2 = C-\frac{1}{{\sigma_0}^2}L(\beta,X)      \\
    \underbrace{\beta^*}_{\textsf{MLE}} & = \arg\max_\beta \log(p(X|\beta)) = \underbrace{\arg\min_\beta L(\beta, X)}_{\textsf{OLSE}}
\end{align*}

\subsection{Bayesianism}