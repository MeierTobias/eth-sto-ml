\section{Linear Regression}
Linear regression is a form of \textbf{statistical learning}.
\subsection{Simple Linear Regression}
For simple linear regression there is only one feature in contrast to multivariate linear regression.

\subsubsection{Dataset}
The set of pairs
\begin{align*}
    (x,y) & \in\mathbb{R}\times\mathbb{R}       \\
    x:    & \text{ input variable, regressor}   \\
    y:    & \text{ output variable, regressand}
\end{align*}
forms the dataset for simple linear regression
\begin{equation*}
    D=\{(x_1,y_1),(x_2,y_2),\dots,(x_n,y_n)\}
\end{equation*}

\subsubsection{Model}
Assuming $D$ is a realization of a sample
\begin{equation*}
    (X_1,Y_1),(X_2,Y_2),\dots,(X_n,Y_n)\thicksim F
\end{equation*}
simple linear regression assumes for the \textbf{model} $\mathcal{H}$ the distribution $F$ as
\begin{align*}
    Y_i        & =\beta_0X_i+\beta^{\prime}_0+\epsilon_i \\
    \epsilon_i & \sim N(0,\sigma^2): \text{ noise}
\end{align*}
where $\beta_0,\beta^{\prime}_0,\sigma\in\mathbb{R}$ are unknown and $\mathcal{H}$ is the set of all functions of the form $f(x)=\beta x+\beta'$.\ \textbf{Justification} for this choice is given by the assumption that the output variable $y$ is proportional to the input variable $x$ plus a base value plus some noise.

\subsubsection{Loss Function}
Given the dataset $D$ and the \textbf{estimator} $f$ we choose the \textbf{mean squared error (MSE)}
\begin{equation*}
    L(D,f)=\frac{1}{n}\sum_{i\leq n}{\left(y_i-f(x_i)\right)}^2
\end{equation*}
as cost function.

\ptitle{Remarks}

\begin{itemize}
    \item Another natural choice would have been $|f(x)-y|$ which is not differentiable.
    \item MSE punishes large deviations more than small ones.
\end{itemize}

\subsubsection{Training Algorithm}
One tries to get the best estimator for $\hat{f}(x)=\hat{\beta}x+\widehat{\beta^{\prime}}$ by finding
\begin{equation*}
    \hat{f}=argmin_{f\in\mathcal{H}}L(D,f)
\end{equation*}

\ptitle{Optimal Parameters}

The parameters minimizing the MSE are given by
\begin{equation*}
    \begin{pmatrix}
        \hat{\beta} \\
        \widehat{\beta'}
    \end{pmatrix}
    ={(\mathbf{X}^T \mathbf{X})}^{-1}\mathbf{X}^T\mathbf{\gamma}
\end{equation*}
where
\begin{equation*}
    \mathbf{X}=
    \begin{pmatrix}
        x_1 & 1 \\
        x_2 & 1 \\
        \vdots  \\
        x_n & 1
    \end{pmatrix},\quad
    \mathbf{\gamma}=
    \begin{pmatrix}
        y_1    \\
        y_2    \\
        \vdots \\
        y_n
    \end{pmatrix}
\end{equation*}

\paragraph{Statistical Testing and Confidence Intervals}

\ptitle{Confidence Intervals}

$\hat{\beta}$ is a statistical estimator of $\beta$. Therefore, one can derive a confidence interval on it.

\ptitle{p-Values}

\begin{itemize}
    \item One can also test with $p$-values.
    \item E.g. $H_0$: $\beta_0=0$. The null hypothesis assumes that there is no correlation between area and house prices.
    \item A low $p$-value indicates a correlation.
\end{itemize}

\ptitle{Remarks}

\begin{itemize}
    \item $\hat{f}$ is called the \textbf{ordinary least-squares estimator (OSLE)}
\end{itemize}

\paragraph[Validation with R2]{Validation with $R^2$ score}
\begin{equation*}
    R^2(D,\hat{f})=1-\frac{MSE(D,\hat{f})}{MSE(D,f_0)}\in\left[-\infty,1\right) %ChkTex 9
\end{equation*}

with the \textbf{dummy estimator}
\begin{equation*}
    f_0=\frac1n\sum_{i}{y_i}
\end{equation*}

\ptitle{Remarks}

\begin{itemize}
    \item Comparing the MSE on the test data is not informative.
    \item Better: The $R^2$ score compares the estimator's performance with the performance of a dummy estimator.
    \item One tries to get as close as possible to $R^2=1$
\end{itemize}

\subsection{Multivariate Linear Regression}
\subsubsection{Dataset}
Similar to simple linear regression but now the input variable is a vector containing all features i.e.

\begin{align*}
    (\mathbf{x},y) & \in\mathbb{R}^d\times\mathbb{R}     \\
    \mathbf{x}:    & \text{ input variable, regressor}   \\
    y:             & \text{ output variable, regressand}
\end{align*}
and
\begin{equation*}
    D=\{(\mathbf{x_1},y_1),(\mathbf{x_2},y_2),\dots,(\mathbf{x_n},y_n)\}
\end{equation*}

