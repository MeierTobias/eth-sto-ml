\section{Classification Trees}
A classification tree
\noindent\begin{equation*}
    t: \mathbb{R}^d \to \{1,\ldots, M\}
\end{equation*}
maps $\mathbb{R}^d$ to labels and consists of
\begin{itemize}
    \item a partition of $\mathbb{R}^d$:
          \noindent\begin{equation*}
              S_1\cup \cdots S_K = \mathbb{R}^d \;:\; S_i\cap S_j=\emptyset , i,j\leq K
          \end{equation*}
    \item An assignment of lables to each partition from a predefined set of classes $\{1,\ldots, M\}$.
    \item A dataset of labeled examples is given by
          \noindent\begin{equation*}
              D=\{(x_{1},y_{1}),\ldots,(x_{n},y_{n})\}\subseteq\mathbb{R}^{d\times M},\qquad x\in \mathbb{R}^d
          \end{equation*}
\end{itemize}

\ptitle{Remarks}

\begin{itemize}
    \item For $\mathbf{x}\in S_i\in\mathbb{R}^d$, $t(\mathbf{x})$ is the label associated with $S_i$.
    \item $t$ induces a partition of $D$ as it divides $D$.
\end{itemize}


\subsection{Entropy}
One way to measure the quality of a tree is by calculating the entropy (randomness) resulting form the partition:
\noindent\begin{equation*}
    H(D)=-\sum_{j\leq M}p_j\log p_j
\end{equation*}

\noindent\begin{align*}
    \mathsf{D}: && H(X)&=-\sum_{x\in X}\mathbb{P}(X=x)\log\mathbb{P}(X=x)\\
    \mathsf{C}: && H(X)&=-\int p(x)\log p(x) dx
\end{align*}

\ptitle{Remarks}

\begin{itemize}
    \item Usually $\log_2$ is used.
    % \item Entropy measures the randomness in a random variable (or dataset).
    \item $H$ is the only function that fulfills
          \begin{itemize}
              \item $H$ is non-negative
              \item Chain rule:
              \noindent\begin{equation*}
                H(X,Y) = H(X) + H(Y|X) \overset{\mathrm{idp.}}{=} H(X) + H(Y)
              \end{equation*}
              \item $X\sim Unif$ maximizes $H$
          \end{itemize}
\end{itemize}

\subsubsection{Entropy for Classification}
For max.\ order in $D$, $t^*$ is the tree that minimizes the loss $L$
\noindent\begin{equation*}
    t^* = \min_t L(D,t) =\min_t \sum_{i=1}^{K} \frac{|D_i|}{|D|}H(D_i)
\end{equation*}
where in $H(D_i)$, $\mathbb{P}(X=x)$ is the relative frequency of label $x$ in $D_i$. $\frac{|D_i|}{|D|}$ is a normalized weight.

\newpar{}
\ptitle{Bias-Variance Tradeoff}
\begin{itemize}
    \item Perfect overfitting: every datapoint gets it's own leaf.
    \item Reduce variance: thresholds e.g.\ on depth, leaf node size.
\end{itemize}

\subsection{Greedy Algorithm}
\ptitle{\code{train(Dataset $D$)}}
\fncode{
    \begin{algorithmic}
        \If{H(D)==0}
        \State{\Return{only exisiting label $y$}}
        \Else{}
        \State{$(f,t) \gets$ partitioning line that minimizes $Q(D,f,t)$}
        \State{$D_\ell, D_r$ $\gets$ partitions resulting from $(f,t)$}
        \If{$f \geq t$}
        \State{\Return{train($D_\ell$)}}
        \Else{}
        \State{\Return{train($D_r$)}}
        \EndIf{}
        \EndIf{}
    \end{algorithmic}
}
% \begin{lstlisting}[style=bright_C++]
% def train(D):
%     if H(D) = 0 then:
%         return  y, where y is the only label 
%         appearing in D. 
%     else:
%         compute the partitioning line (f,t) 
%         that minimizes Q(D,f,t) . 
%         let D_l and D_r be the parts resulting from (f,t)
%         return (f>=t)? train(D_l) : train(D_r)
% \end{lstlisting}

\ptitle{Remarks}

\begin{itemize}
    \item A partition line is a pair $(f,t)$ where $f$ is a feature and $t$ is a threshold value.
    \item $Q(D,f,t)=\frac{|D_\ell|}{|D|}H(D_l)+\frac{|D_r|}{|D|}H(D_r)$ is the total entropy (imposed by a partitioning line) to be minimized (2D case).
\end{itemize}