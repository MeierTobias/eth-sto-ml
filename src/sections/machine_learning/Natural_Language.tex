\section{Natural Language Processing}
\subsection{Preprocessing}
\subsubsection{Tokenization}
Tokenization is used to split up words or other input data into smaller \textbf{tokens}. This ensures that the inputs are more regular in size and introduces inter-word context.

\subsection{Embeddings}

\subsubsection{Bag of Words}
BoW is a simple way of representing text, by counting the number of occurences of the $n-1$ most frequent words (all other words are assigned `UNK').
This results in a $n$ dimensional vector representation of the text sequence.
\begin{itemize}
    \item [+] Simple
    \item [-] No notion of dependencies between words (focuses only on frequency of words)
    \item [-] Sparsity 
\end{itemize}
\subsubsection{Word Embeddings}
In contrast to BoW \textit{word embeddings} take the context of the word/token into account (but \textbf{not} their sequence).
\begin{itemize}
    \item Similar words have similar embeddings (distributional hypothesis) as one takes into accout the surrounding.
    \item The general strategy of Continous BoW (CBOW) and skip-gram is to extract embeddings as a by-product of training a NN:
    \begin{enumerate}
        \item train a NN for a prediction task (predict gap or surrounding of a word)
        \item use some of the trained layer weights as dense word embeddings
    \end{enumerate}
    \item The main components in CBOW and skip-gram are:
    \begin{itemize}
        \item $\mathbf{x}_w \in \mathbb{R}^d$: embedding of target word
        \item $\mathbf{z} _w$ in $\mathbb{R}^d$: embedding of given word(s) (summed up for CBOW)
        \item $\mathcal{V}$: Vocabulary
        \item $d$: embedding dimension (usually $\in [50\dots500]$)
    \end{itemize}
\end{itemize}

\paragraph{CBoW}
\begin{itemize}
    \item context $\to$ word
    \item faster and better for frequent words
\end{itemize}

The functional one wants to maximize is:
\noindent\begin{gather*}
    J_\theta^{\mathsf{CBOW}}                                               = \sum_{t}\log\left(p(w_t|w_{t-c},\ldots, w_{t-1},w_{t+1},\ldots, w_{t+c})\right)                                   \\
    p(\underbrace{v}_{\textsf{target}} | \underbrace{w}_{\textsf{given}})  = \frac{\exp(\mathbf{x}_v^{\mathsf{T}}\mathbf{z}_w)}{\sum\limits_{u\in \mathcal{V}} \mathbf{x}_u^{\mathsf{T}}\sum\mathbf{z}_w} \quad \mathcal{V}: \text{Vocabulary}
\end{gather*}

\newpar{}
\ptitle{Example Calculation}
\begin{center}
    \includegraphics[width=\linewidth]{nlp_cbow.png}
\end{center}
\begin{enumerate}
    \item create one-hot encoding of inputs
	\item multiply each word by $W_1$ and sum the results 
	\item multiply the sum by $W_2$
	\item apply softmax and round to get one-hot encoding of gap
\end{enumerate}

\paragraph{Skip-Gram}
\begin{itemize}
    \item Word $\to$ Context
    \item better for smaller datasets and infrequent words
\end{itemize}

One wants to maximize the functional
\noindent\begin{gather*}
    J_{\theta}^{\mathsf{SG}} = \sum_{t}\sum_{\overset{l=-c}{l\neq 0}}^{c} \log(p(w_{t+l}|w_t))\\
    p(\underbrace{v}_{\textsf{target}} | \underbrace{w}_{\textsf{given}})  = \frac{\exp(\mathbf{x}_v^{\mathsf{T}}\mathbf{z}_w)}{\sum\limits_{u\in \mathcal{V}} \mathbf{x}_u^{\mathsf{T}}\mathbf{z}_w} \quad \mathcal{V}: \text{Vocabulary}
\end{gather*}

\newpar{}
\ptitle{Example Calculation}
\begin{center}
    \includegraphics[width=\linewidth]{nlp_skip.png}
\end{center}
The embedding of a one-hot encoded word can then be obtained by multiplication with the trained weight matrix $W_1$.

\subsubsection{Sequences of Words}
\ptitle{n-Grams}
N-grams are $n$ consecutive words/tokens in a text.
\begin{itemize}
    \item \textbf{Unigrams} treat each word independently
    \item \textbf{Bigram}, \textbf{trigrams} etc.\ treat 2,3,$\ldots$ words in combination
\end{itemize}
\textbf{Remarks}
\begin{itemize}
    \item The combination of words grows with $\mathcal{O}(c^n)$
    \item An \textbf{n-gram language model} predicts the probability of word $n$ given the words $0,\dots,n-1$. Two of the main issues are:
    \begin{itemize}
        \item Sparsity: sentences that are rare in the corpus have $P=0$ to be predicted. For sentences not given in the corpus one can't calculate $P$ at all.
        \item Storage: if the corpus is large storing all the possbile word permutations becomes hard.
    \end{itemize}
\end{itemize}

\newpar{}
See RNN Section~\ref{sec:RNN}

\subsubsection{Position Encoding}
Through position encoding, information about the position of a word/token can be incorporated. As a result, the same word/token can have a differnet representation/embedding at different positions.
\newpar{}
\ptitle{Sine/Cosine Positional Encoding}

Usually one uses sine and cosine functions to add positional information to the encoding.
\begin{align*}
    PE_{(pos,2i)}&=\sin\left(\frac{pos}{10000^{\frac{2i}{d_{model}}}}\right)\\
    PE_{(pos,2i+1)}&=\cos\left(\frac{pos}{10000^{\frac{2i}{d_{model}}}}\right)
\end{align*}
Remarks:
\begin{itemize}
    \item sine is used for even positions, cosine for odd occurences
    \item $d_{model}$ is the number of entries in the embedding vector
    \item $pos$ is the word's position in the sentence
    \item overall-embedding of a word is given by $E+PE$ where $E$ is the common word embedding
    \item two different words have the same $PE$ if they reside at the same position within their sentences
    \item nearby words have similar positional encodings
\end{itemize}

\subsection{Language Models}
\subsubsection{Attention}
\textit{Attention is a fuzzy, differentiable, vectorized dictionary look-up.}

\newpar{}
Attention enhances the performance by enabling long-range dependencies without sequential processing.

\newpar{}
A attention block takes as inputs:
\begin{itemize}
    \item \textbf{Query}: encodes a certain ``question'' (e.g.\ ``What did the cat catch'', or a word to translate) in a lower dimension than the embedding vector. Calculated using the matrix $W_Q$.
    \item \textbf{Key}: encodes ``answer'' to a certain query (e.g.\ ``blindworm'' because it was caught by the cat, or a word in source language) in a lower dimension than the embedding vector. Calculated using the matrix $W_K$. Query and key vector (i.e.\ question and answer) match well if they are well-aligned i.e.\ have a large dot product.
    \item \textbf{Value}: weighted value (e.g.\ weighted word in target language) encodes how to update one word based on a previous word, clearly in the same dimension as the embedding vector. Calculated using the matrix $W_V$. Weighted by the query-key dot products.
\end{itemize}
Whe matrices representing query, key and value are learned by the model.

\newpar{}
\ptitle{Scaled Dot-Product Attention\;\; Multi-Head Attention}
\begin{center}
    \includegraphics[width=\linewidth]{nlp_attention.png}
\end{center}
Remarks:
\begin{itemize}
    \item Scaled dot-product attention encodes one head of attention.
    \item Multi-head attention contains multiple layers of scaled dot-product attention.
    \item To adapt the attention to the task one applies a linear transformation to all 3 inputs and the output. 
\end{itemize}

\begin{examplesection}[Calculating Scaled Dot-Product Attention]
    Given
    \noindent\begin{equation*}
        \mathbf{X}\in \mathbb{R}^{n_{\mathsf{words}} \times d_{\mathsf{rep}}}\qquad (\mathbf{W}_Q, \mathbf{W}_K, \mathbf{W}_V)\in \mathbb{R}^{d_{\mathsf{rep}}\times n_{\mathsf{words}}}
    \end{equation*}
    scaled dot-product attention is given by
    \noindent\begin{equation*}
        \mathrm{softmax}\left(\frac{\mathbf{QK}^{\mathsf{T}}}{\sqrt{d_{\mathsf{rep}}}}\right) \mathbf{V},\qquad
        \begin{cases}
            \mathbf{Q} = \mathbf{XW}_Q \\
            \mathbf{K} = \mathbf{XW}_K \\
            \mathbf{V} = \mathbf{XW}_V
        \end{cases}
    \end{equation*}
    \newpar{}
    \ptitle{Remarks}
    \begin{itemize}
        \item The $\mathbf{Q}$, $\mathbf{K}$, $\mathbf{V}$ matrices are filled with all the query, key and value vectors. 
        \item Attention is added to a word embedding to put weight of the other words to it.
        \item $\sqrt{d_{\mathsf{rep}}}$ is imposed to enhance numerical stability.
    \end{itemize}
\end{examplesection}

\paragraph{Self- and Cross-Attention}

\ptitle{Self-Attention}:

\begin{itemize}
    \item Words can correspond to different words within the same sequence. 
    \item The attention block uses embedding vectors from only one space, e.g.\ one language in a GPT.
\end{itemize}

\newpar{}
\ptitle{Cross-Attention}:

\begin{itemize}
    \item Words can have different meaning between different sequences:
    \begin{center}
        \includegraphics[width=.3\linewidth]{nlp_cross_attention.png}
    \end{center}
    \item The attention block uses embedding vectors from multiple spaces, such as german and french language in a translator.
\end{itemize}

\subsubsection{Transformers}
\newpar{}
\ptitle{Key Properties}
\begin{itemize}
    \item a type of NN architecture
    \item no exploding or vanishing gradients
    \item can be trained in parallel
    \item good at handling long-range dependencies in text and images (enabled through attention blocks)
\end{itemize}
\paragraph{Basic Architecture}
\begin{center}
    \includegraphics[width=\linewidth]{nlp_transformer.png}
\end{center}

\newpar{}
\ptitle{Residual Connections}
\begin{itemize}
    \item When an input signal propagates through a very deep NN, almost no characteristics of the input signal will be preserved once the signal reaches the final layers.
    \item Similarly, gradients vanish during backpropgation and therefore the update to the early layer's gradients becomes uninformative.
    \item Residual connections and are used to mitigate these problems by preserving some of the orginal information for later stages in a calculation. Note that one splits a deep NN into more shallow blocks.
\end{itemize}

\newpar{}
\ptitle{Remarks}
\begin{itemize}
    \item Note that there is a self-attention block in the encoder but a self- and a cross-attention block in the decoder.
    \item % TODO: Intuition for the cross-attention block. Is this to preserve dependencies between the words?
\end{itemize}

\paragraph{Attention in Transformers}
\newpar{}
\ptitle{High-Level Function of Attention Blocks in Transformers}

Attention blocks
\begin{itemize}
    \item give the decoder access to the entire input 
    \item provide weights for the decoder to decide on which input word is how important for the next output
    \item calculate what one needs to add to a generic embedding to get an embedding representing context
    \item pass information from many words' embeddings to the embedding of one word
\end{itemize}

\newpar{}
\ptitle{Nomenclature}
\begin{itemize}
    \item \textit{Initial embedding}: encodes a word just considering itself and a positional encoding, ignoring context.
    \item \textit{Refined embedding}: embedding that encodes additional context information.
    \item \textit{Head of attention}: encodes a certain semantic relation (e.g.\ how adjectives specify the meaning of a noun more precisely) to update embeddings based on context.
    \item \textit{Change in embedding}: change in embedding proposed by a certain head of attention.
\end{itemize}

\begin{examplesection}[Creating Scaled Dot-Product Self Attention for a Multi-Head Attention Block]
    Assuming a sample sentence ``The cat caught the blindworm but not the anaconda'' and starting with the head of attention encoding ``What did the cat catch'', one proceeds as follows:
    \begin{enumerate}
        \item Calculate initial embeddings of all words.
        \item Calculate dot products of query and key vectors
        \begin{itemize}
            \item Large dot products indicate a strong connection between two words (e.g.\ ``cat'', ``blindworm'').
        \end{itemize}
        \item Mask dot products representing the influence from \textbf{later} words on earlier words by ``$-\infty$'' ($0$ influence in softmax), then apply softmax.
        \begin{itemize}
            \item One normalizes the dot products to $[0,1]$ using softmax to get a probability mass.
            \item The magnitude of the masked and ``softmaxed'' dot product specifies how much one wants to update the current embedding of one word based on the embedding of a previous word.
        \end{itemize}
        \item Calculate the value vector for each word, weigh it by the query-key dot products with the other words and add it to the other words (in practise vectors get merged to matrices).
        \begin{itemize}
            \item Because the query-key dot product of ``blindworm'' and ``cat'' is large, one would add much of the ``blindworm'' embedding to the ``cat'' embedding.
            \item Other words, such as ``anaconda'' have a small query-key dot product with ``cat''. Therefore, one adds little or nothing of the ``anaconda'' embedding to the ``cat'' embedding.
            \item Remember that this only holds for the ``What did the cat catch'' head of attention. E.g.\ in the ``by what was the cat caught'' head of attention, the query-key dot product of cat and ``anaconda'' would possibly be large.
        \end{itemize}
        \item Repeat the procedure for all other heads of attention.
    \end{enumerate}
\end{examplesection}

\paragraph{Encoder}
In the encoding part, information in the text sequence is \textit{encoded} into \textbf{representation vectors}.
\newpar{}
\ptitle{Training}
\begin{itemize}
    \item Use the complete sequence for training
    \item Trained based on \textbf{word masking} (self-supervised):
          \begin{itemize}
              \item Replace some words with \fncode{<mask>} and some with random words
              \item Model needs to find the masked/replaced words
          \end{itemize}
\end{itemize}
\begin{center}
    \includegraphics[width=.4\linewidth]{nlp_enc_training.png}
\end{center}


\paragraph{Decoder}

Based on the representations of the encoder, the decoder generates output from the \textbf{representation vectors}
\newpar{}
\ptitle{Training}
\begin{itemize}
    \item Only past sequence matters (causal attention)
    \item Training based on \textbf{next word prediction} (self-supervised)
\end{itemize}
\begin{center}
    \includegraphics[width=.4\linewidth]{nlp_dec_training.png}
\end{center}

\subsubsection{Multimodel Transformers}
Transformers are not limited to text sequences, tokens can also be generated from audio, images or other sources and their combination.
