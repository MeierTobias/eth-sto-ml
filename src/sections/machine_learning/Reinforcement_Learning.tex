\section{Reinforcement Learning}

\subsection{Markov Decision Process}

A discrete Markov Decision Process consists of:
\begin{itemize}
    \item $S$ a finite set of states.
    \item $A$ a finite set of actions.
    \item $P$ a set of transition probabilities. \\ For $s \in S$ and $a \in A$, $P(\cdot | s,a)$ is a distribution over $S$. All possible $P(\cdot | s,a)$ must sum up to 1.
    \item $R(s, a, s')$ a reward function assigning each triple $s, a, s' \in S \times A \times S$ to a real number.
    \item $\gamma \in (0,1)$ is the discount factor. % TODO: Finish this
\end{itemize}

\subsection{Policies}

A policy is a function $\pi$ that maps each state $s \in S$ to a distribution $\pi(\cdot|s)$ over the set $A$ of actions.
\newpar{}
The goal is to find the policy that maximizes the reward.

\subsection{Bellman Equation}
To find the optimal policy one can determine the expected reward $V_{t}^{\pi}(s)$ when following a certain policy $\pi$ for $t$ steps given the initial state $s$.

In general the value function is given by
\begin{align*}
    V_{t}^{\pi}(s) & = \sum_{a\in A} \sum_{s' \in S} \pi(a|s)P(s'|s,a)\left(R(s,a,s')+V_{t-1}^{\pi}(s')\right)                        \\
                   & = \mathbb{E}_{a \sim \pi(\cdot|s)} \mathbb{E}_{s' \sim P(\cdot|s, a)} \left[R(s,a,s') + V_{t-1}^{\pi}(s')\right]
\end{align*}

To get the asymptotic limit when $t \to \infty$ one has to incorporate the discount factor $\gamma$.
\begin{align*}
    V^{\pi}(s) & = \mathbb{E}_{a \sim \pi(\cdot|s)} \mathbb{E}_{s' \sim P(\cdot|s, a)} \left[R(s,a,s') + \gamma V^{\pi}(s')\right] \\
               & = \mathbb{E}\left[\sum_{t\in \mathbb{N}}\gamma^t R(s_t,a_t,s_{t+1})\right]
\end{align*}
where
\begin{equation*}
    a_t \sim \pi(\cdot | s_t) \quad \text{and} \quad s_{t+1}\sim P(\cdot | s_t,a_t)
\end{equation*}

In addition the Action-Value function gives the reward that can get attained by following a policy $\pi$ from a state $s \in S$,
after taking action $a \in A$.
\begin{equation*}
    Q^\pi(s,a) = \mathbb{E}_{s' \sim P(\cdot|s,a)}\left[R(s,a,s')+\gamma \mathbb{E}_{a'\sim \pi(\cdot|s')} Q^\pi(s',a')\right]
\end{equation*}
and reversed
\begin{equation*}
    V^\pi(s) = \mathbb{E}_a \left[ Q^\pi (s,a)\right]
\end{equation*}

\newpar{}

The optimal policy $\pi^*$ is the one that picks the action that yields the highest value.
\begin{align*}
    V^*(s)   & = \max_{a\in A}\mathbb{E}_{s'\sim P(\cdot|s,a)}\left[R(s,a,s')+\gamma V^* (s')\right]       \\
    Q^*(s,a) & = \mathbb{E}_{s'\sim P(\cdot|s,a)}\left[R(s,a,s') + \gamma \max_{a'\in A}Q^* (s',a')\right]
\end{align*}

\subsection{Robbins-Monro Algorithm}

\begin{enumerate}
    \item Choose a learning rate $\alpha_1, \alpha_2, \ldots$
    \item Make an initial guess $x_0$
    \item For $t={0,1,\ldots}$ compute $x_{t+1} \leftarrow x_t - \alpha_t \Delta(x_t)$
\end{enumerate}

\newpar{}
\ptitle{Remarks:}
\begin{itemize}
    \item If $\sum_{t}\alpha_t = \infty$ and $\sum_{t}\alpha^2_t<\infty$, \\ then $\lim_{t\to\infty}\mathbb{P}(|x_t - x^*|>\epsilon) = 0$, for any $\epsilon > 0$.
    \item Because of this, $\alpha_t = \frac{1}{t}$. However, in practice $\alpha_t = 0.001$.
\end{itemize}

\subsection{Q-Learning}

To find the optimal policy one can rewrite the optimal Action-Value function to
\begin{equation*}
    0 = \mathbb{E}_{s'\sim P(\cdot|s,a)}\left[R(s,a,s') + \gamma \max_{a'\in A}Q^* (s',a') - Q^*(s,a)\right]
\end{equation*}
and apply the Robbins-Monro algorithm.


\newpar{}
In detail:
\begin{enumerate}
    \item Initialize arbitrarily a table $Q$ indexed by states and actions.
    \item Pick the learning rates $\alpha_0, \alpha_1, \ldots$
    \item Choose an initial state $s$.
    \item For $t = 1, 2, \ldots$
          \begin{enumerate}
              \item Choose an action $a = \underset{a'\in A}{\mathrm{argmax}}\,Q(s,a')$.
              \item Take action $a$, observe the new state $s'$ and the reward obtained $r=R(s,a,s')$.
              \item $Q(s,a) \leftarrow Q(s,a) - \alpha_t \left(r + \gamma\,\underset{a'\in A}{\max}\,Q(s',a')-Q(s,a)\right)$
              \item $s \leftarrow s'$
          \end{enumerate}
\end{enumerate}

\subsection{Policy Gradient Method}
For a parametric family of policies $\Pi = \{\pi_\theta:\theta\in\Theta\}$, for example, if the set of states is $\mathbb{R}^d$ and
the set of actions is $A = \{a_1, a_2, \ldots, a_m\}$, then a policy can be a neural network that takes as input a state and outputs a discrete distribution over $A$. Here, $\theta$ would be the values of the weights and biases of the neural network.
\newpar{}
The goal is then to find $\theta^* = \underset{\theta\in\Theta}{argmax}\,V^{\pi_\theta}(s_0)$.
\newpar{}
In the neural network case one can use gradient descent for the objective function $V^{\pi_\theta}(s_0)$ to determine the best weights $\theta$.
