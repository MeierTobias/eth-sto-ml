\section{Reinforcement Learning}

% \subsection{Notation}
% \renewcommand{\arraystretch}{1.3}
% \setlength{\oldtabcolsep}{\tabcolsep}\setlength\tabcolsep{6pt}

% \begin{tabularx}{\linewidth}{@{}ll@{}}
% $t$                & discrete timestep                              \\
% $s$                & state in $\mathcal{S}$                         \\
% $S_t$              & state at $t$                                   \\
% $a$                & action in $\mathcal{A}$                        \\
% $A_t$              & action at $t$                                  \\[.5em]
% $r$                & reward in $\mathcal{R}$                        \\
% $R_t$              & reward at $t$                                  \\
% $G_t$              & return at $t$                                  \\[.5em]
% $\pi$              & policy                                         \\
% $\pi(s)$           & action taken in $s$ under deterministic policy \\
% $\pi(a|s)$         & prob.\ of taking $a$ in $s$ under $\pi$        \\
% $p(s',r|s,a)$      & transition probability in $\mathcal{P}$        \\[.5em]
% $v_{\pi}(s)$       & value of state under policy $\pi$              \\
% $V_{t}^{\pi}(s)$   & estimate of $v_{\pi}(s)$ at $t$                \\
% $q_{\pi}(s,a)$     & action value of state under policy $\pi$       \\
% $Q_{t}^{\pi}(s,a)$ & estimate of $q_{\pi}(s, a)$ at $t$             \\
% \end{tabularx}

% \renewcommand{\arraystretch}{1}
% \setlength\tabcolsep{\oldtabcolsep}

\subsection{Markov Decision Process}

A finite and discrete Markov Decision Process consists of:

\begin{itemize}
    \item $\mathcal{S}$ a finite set of states.
    \item $\mathcal{A}$ a finite set of actions.
    \item $\mathcal{P}$ a set of transition probabilities. \\ For $s \in \mathcal{S}$ and $a \in \mathcal{A}$, $p(\cdot | s,a)$ is a distribution over $\mathcal{S}$. All possible $p(\cdot | s,a)$ must sum up to 1.
    \item $R(s, a, s')$ a reward function assigning each triple $s, a, s' \in \mathcal{S} \times \mathcal{A} \times \mathcal{S}$ to a real number.
    \item $\gamma \in (0,1)$ is the discount factor.
\end{itemize}

\subsection{Policies}

A policy is a function $\pi$ that maps each state $s \in \mathcal{S}$ to a distribution $\pi(\cdot|s)$ over the set $\mathcal{A}$ of actions.
\newpar{}
The goal is to find the policy that maximizes the reward.

\subsection{Bellman Equation}
\subsubsection{Value Function}
$V_{t}^{\pi}(s)$ is the value when following a certain policy $\pi$ for $t$ steps given the initial state $s$.

\newpar{}
\ptitle{Finite Horizon}

In general the value function is given by
\begin{align*}
    V_{t}^{\pi}(s) & = \sum_{a\in A}\pi(a|s) \sum_{s' \in S} p(s'|s,a)\left(R(s,a,s')+V_{t-1}^{\pi}(s')\right)                                                   \\
                   & = \begin{cases}
                           \mathbb{E}_{a \sim \pi(\cdot|s)} \mathbb{E}_{s' \sim p(\cdot|s, a)} \left[R(s,a,s') + V_{t-1}^{\pi}(s')\right] & t > 0 \\
                           0                                                                                                              & t=0
                       \end{cases}
\end{align*}

\newpar{}
\ptitle{Infinite Horizon}

To get the asymptotic limit when $t \to \infty$, one has to incorporate the discount factor $\gamma$.
\begin{align*}
    V^{\pi}(s) & = \mathbb{E}_{a \sim \pi(\cdot|s)} \mathbb{E}_{s' \sim p(\cdot|s, a)} \left[R(s,a,s') + \gamma V^{\pi}(s')\right]                      \\
               & = \mathbb{E}\left[\sum_{t\in \mathbb{N}}\gamma^t R(s_t,a_t,s_{t+1})\right] = \underbrace{\mathbb{E}\left[G_t\right]}_{\textsf{Return}}
\end{align*}
where $a_t \sim \pi(\cdot | s_t) \text{ and } s_{t+1}\sim p(\cdot | s_t,a_t)$

\subsubsection{Action Value Function}
In addition the Action-Value function gives the reward that can get attained by following a policy $\pi$ from a state $s \in S$,
after taking action $a \in A$.
\begin{equation*}
    Q^\pi(s,a) = \mathbb{E}_{s' \sim p(\cdot|s,a)}\left[R(s,a,s')+\gamma \mathbb{E}_{a'\sim \pi(\cdot|s')} Q^\pi(s',a')\right]
\end{equation*}
and reversed
\begin{equation*}
    V^\pi(s) = \mathbb{E}_a \left[ Q^\pi (s,a)\right]
\end{equation*}

\subsubsection{Optimal Policy}
The optimal policy $\pi^*$ is the one that picks the action that yields the highest value.
\begin{align*}
    Q^*(s,a) & = \mathbb{E}_{s'\sim p(\cdot|s,a)}\left[R(s,a,s') + \gamma \max_{a'\in A}Q^* (s',a')\right]     \\
    V^*(s)   & = \max_{a\in \mathcal{A}}\mathbb{E}_{s'\sim p(\cdot|s,a)}\left[R(s,a,s')+\gamma V^* (s')\right] \\
             & = \max_{a\in \mathcal{A}} Q^{*}(s,a)                                                            \\  
    \pi^{*}(s)  & = \arg \max_{a\in \mathcal{A}} Q^{*}(s,a)
\end{align*}

\subsection{Robbins-Monro Algorithm}

\begin{enumerate}
    \item Choose a learning rate $\alpha_1, \alpha_2, \ldots$
    \item Make an initial guess $x_0$
    \item For $t={0,1,\ldots}$ compute $x_{t+1} \leftarrow x_t - \alpha_t \Delta(x_t)$
\end{enumerate}

\newpar{}
\ptitle{Remarks:}
\begin{itemize}
    \item If $\sum_{t}\alpha_t = \infty$ and $\sum_{t}\alpha^2_t<\infty$, \\ then $\lim_{t\to\infty}\mathbb{P}(|x_t - x^*|>\epsilon) = 0$, for any $\epsilon > 0$.
    \item Because of this, $\alpha_t = \frac{1}{t}$. However, in practice $\alpha_t = 0.001$.
\end{itemize}

\subsection{Q-Learning}

To find the optimal policy one can rewrite the optimal Action-Value function to
\begin{equation*}
    0 = \mathbb{E}_{s'\sim p(\cdot|s,a)}\left[R(s,a,s') + \gamma \max_{a'\in A}Q^* (s',a') - Q^*(s,a)\right]
\end{equation*}
and apply the Robbins-Monro algorithm.


\newpar{}
In detail:
\begin{enumerate}
    \item Initialize arbitrarily a table $Q$ indexed by states and actions.
    \item Pick the learning rates $\alpha_0, \alpha_1, \ldots$
    \item Choose an initial state $s$.
    \item For $t = 1, 2, \ldots$
          \begin{enumerate}
              \item Choose an action $a = \underset{a'\in A}{\mathrm{argmax}}\,Q(s,a')$. Usually the action is chosen in an \textit{epsilon-greedy} way
              \begin{itemize}
                \item Exploitation: With probability of $1-\varepsilon$, select action with highest estimated Q-value
                \item Exploration: With probability $\varepsilon$, randomly select an action
              \end{itemize}
              \item Take action $a$, observe the new state $s'$ and the reward obtained $r=R(s,a,s')$.
              \item $Q(s,a) \leftarrow Q(s,a) + \alpha_t \left(r + \gamma\,\underset{a'\in A}{\max}\,Q(s',a')-Q(s,a)\right)$
              \item $s \leftarrow s'$
          \end{enumerate}
\end{enumerate}

\subsection{Policy Gradient Method}
For a parametric family of policies $\Pi = \{\pi_\theta:\theta\in\Theta\}$, for example, if the set of states is $\mathbb{R}^d$ and
the set of actions is $A = \{a_1, a_2, \ldots, a_m\}$, then a policy can be a neural network that takes as input a state and outputs a discrete distribution over $A$. Here, $\theta$ would be the values of the weights and biases of the neural network.
\newpar{}
The goal is then to find $\theta^* = \underset{\theta\in\Theta}{\arg\max}\,V^{\pi_\theta}(s_0)$.
\newpar{}
In the neural network case one can use gradient descent for the objective function $V^{\pi_\theta}(s_0)$ to determine the best weights $\theta$.
