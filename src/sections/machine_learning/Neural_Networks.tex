\section{Neural Networks (NN)}

\renewcommand{\arraystretch}{1.3}
\setlength{\oldtabcolsep}{\tabcolsep}\setlength\tabcolsep{3pt}
{\footnotesize
    \begin{tabularx}{\linewidth}{@{}p{0.2\linewidth}llllX@{}}
        Task                              & Label $y$             & Estimator $\widehat{y}$ & Est.\ distr. & Output L. & Loss \\
        \cmidrule{2-6}
        Binary\newline Classification     & $\in \{0,1\}$         & $P(y=1|\Theta)$         & Bernoulli    & Sigmoid   & BCE  \\
        Multiclass\newline Classification & $\in \{1,., K\}$ & $P(y=k|\Theta)$         & Categorical  & Softmax   & CE   \\
        Regression                        & $\in \mathbb{R}$      & $\in \mathbb{R}$        & Gaussian     & Linear    & MSE
    \end{tabularx}}

\renewcommand{\arraystretch}{1}
\setlength\tabcolsep{\oldtabcolsep}

\subsection{Perceptrons}
A perceptron corresponds to a single-layer NN
\begin{align*}
    a           & =\sum_{i=0}^M w_i\cdot x_i= \mathbf{w}\cdot \mathbf{x} \\
    \widehat{y} & =\sigma(a)
\end{align*}
where the bias term is usually included as $x_0=1,w_0$.
\subsection{Deep Feedforward Networks}

\begin{center}
    \includegraphics[width=0.65\linewidth]{deep_feedforward_network.png}
\end{center}

% $\mathbf{x}$ is the input column vector of size $M+1$ where $M$ is the dimension of \textbf{one} input data point and the first element $x_0 = 1$ which is used to include the bias in the weighting matrix.
% \newpar{}
% The output layer $\hat{\mathbf{y}}$ is a column vector of the size $L$ where $L$ matches the dimensions of \textbf{one} ground truth data point.

% \newpar{}
\ptitle{Pre-Activation}
\begin{equation*}
    \mathbf{a} = \mathbf{W} \mathbf{x}
\end{equation*}

$\mathbf{W} \in \mathbb{R}^{N\times (M+1)}$ where $N$ is the width of the next layer (number of neurons in the next layer without the $z_0$ element).

\newpar{}
\ptitle{Activation}
\begin{equation*}
    \mathbf{z} = \sigma(\mathbf{a})
\end{equation*}
The activation function applies a (non-linear) transformation to the pre-activation vector.

\newpar{}
\ptitle{Remark:}
\begin{itemize}
    \item If there is more than one hidden layer: multi-layer neural network or multi-layer perceptron (MLP).
    \item $\sigma(\cdot)$ can be chosen differently for each layer (but not within a layer).
    \item Fully connected NN are highly symmetric. The symmetry is broken through random initialization of the weights.
\end{itemize}


\subsubsection{Activation Functions}

\renewcommand{\arraystretch}{1.3}
\setlength{\oldtabcolsep}{\tabcolsep}\setlength\tabcolsep{6pt}

\begin{tabularx}{\linewidth}{@{}lXp{0.25\linewidth}@{}}
    Sigmoid & 
    {\begin{equation*}
                 \sigma(x) = \frac{1}{1+e^{-x}}
             \end{equation*}}
            & \includegraphics[width=0.8\linewidth, align=t]{activation_sigmoid.png} \\
    
    Tanh    & 
    {\begin{equation*}
                 \sigma(x) = \tanh(x)
             \end{equation*}}
            & \includegraphics[width=0.8\linewidth, align=t]{activation_tanh.png}    \\
    
    ReLU    & 
    {\begin{equation*}
                 \sigma(x) = \begin{cases}
                    0, & x < 0     \\
                    x, & x \geq 0
                \end{cases}
             \end{equation*}}
            & \includegraphics[width=0.8\linewidth, align=t]{activation_ReLU.png}    \\
    PReLU   & 
    {\begin{equation*}
                 \sigma(x) = \begin{cases}
                    0, & a*x < 0   \\
                    x, & x \geq 0
                \end{cases}
             \end{equation*}}
            & \includegraphics[width=0.8\linewidth, align=t]{activation_PReLU.png}
\end{tabularx}
\begin{tabularx}{\linewidth}{@{}lXp{0.25\linewidth}@{}}
    ELU & 
    {\begin{equation*}
                 \sigma(x) = \begin{cases}
                    0, & a(e^x -1) < 0 \\
                    x, & x \geq 0
                \end{cases}
             \end{equation*}}
        & \includegraphics[width=0.8\linewidth, align=t]{activation_ELU.png}
\end{tabularx}

\ptitle{Softmax}
\begin{gather*}
    \sigma(x_i) = \frac{e^{x_i}}{\sum_{j=1}^{J}e^{x_j}}, \qquad \mathbf{x} \in \mathbb{R}^J \quad J=\text{number of classes}
\end{gather*}
\begin{itemize}
    \item Dependency between all neurons of a layer.
    \item The $\sigma(x_i)$ Sum up to $1$.
    \item Continuous, differentiable version of argmax.
\end{itemize}

\subsubsection{Universal Approximation Theorem}
A single-layer NN with a non-linear activation function and a sufficient number of neurons can approximate \textbf{any continuous function}.
\newpar{}
\ptitle{Using Multiple Layers}
\begin{itemize}
    \item Efficiency: Using more layers can reduce number of neurons per layer: computationally more efficient.
    \item Complexity: Deeper NNs can learn more complex data, perform better and may overfit less compared to wide ones.
    \item Training dynamics: multi-layer NNs train faster and generalize better on real-world datasets.
\end{itemize}

\renewcommand{\arraystretch}{1}
\setlength\tabcolsep{\oldtabcolsep}

\subsection{Gradient-Based Optimization}

To learn/train the weights of the NN a gradient descent method with respect to the error metric (cost function) is used. In other words, the estimation for an input data point is compared with the ground truth and the resulting gradient to minimize the error is determined through back-propagation. This gradient is then used to make a small step ($\eta$) into that direction (all weights are adjusted according to the gradient).
\begin{equation*}
    \mathbf{w}_{t+1} = \mathbf{w}_t - \eta \nabla_w C(y,\hat{y})
\end{equation*}
where $\eta > 0$ is the learning rate.

This process is then repeated multiple times.

\subsubsection{Cost Functions}
Commonly used cost functions are:
\newpar{}
\ptitle{Binary Cross-Entropy} for binary classification
\begin{equation*}
    C(y,\hat{y}) = -y\log(\hat{y})-(1-y)\log(1-\hat{y})
\end{equation*}

\newpar{}
\ptitle{Cross-Entropy} for multi-class classification
\begin{equation*}
    C(y,\hat{y}) = - \sum_{k}y_k\log(\hat{y}_k)
\end{equation*}

\newpar{}
\ptitle{Mean Square Error (MSE)} for regression
\begin{equation*}
    C(y,\hat{y}) = {(y - \hat{y})}^2
\end{equation*}

\newpar{}
\ptitle{Remarks}
\begin{itemize}
    \item The estimators are probabilistic. Discrete labels obtained via argmax/rounding.
\end{itemize}

\subsubsection{Backpropagation}

\ptitle{Scalar Chain Rule}
\begin{equation*}
    \frac{dz}{dx} = \frac{dz}{dy}\frac{dy}{dx}
\end{equation*}

\begin{center}
    \includegraphics[width=0.4\linewidth]{deep_feedforward_network_bb_scalar.pdf}
\end{center}

\ptitle{Vector Chain Rule}
\begin{gather*}
    \frac{\partial z}{\partial x_i} = \sum_{j}\frac{\partial z}{\partial y_j}\frac{\partial y_j}{\partial x_i} \\
    \nabla_x z = {\left(\frac{\partial \mathbf{y}}{\partial \mathbf{x}}\right)}^T \nabla_y z = \sum_{j}(\nabla_x y_j)\frac{\partial z}{\partial y_j}
\end{gather*}

\begin{center}
    \includegraphics[width=0.4\linewidth]{deep_feedforward_network_bb_vector.pdf}
\end{center}

%% Nice to have but probably not used on test
% \ptitle{Tensor Chain Rule}
% \begin{equation*}
%     \nabla_x z = \sum_{j}(\nabla X Y_j)\frac{\partial z}{\partial Y_j}
% \end{equation*}

\subsubsection{Stochastic Gradient Descent (SGD)}

Same as gradient descent but the gradient is estimated by using only a few samples (mini-batch) rather then the whole dataset.
The basic idea is that it is better to do quick unprecise steps then slow and precise ones because the standard error scales with $\frac{1}{\sqrt{n}}$ while the computational effort grows with $n$.
Additionally, redundancies in training data are handled more efficienctly.
% The mini-batch size should be chosen according to your GPU capabilities.

\subsection{Regularization}

Regularization provides methods to reduce overfitting to the training data and therefore generalize better.

The most common ones are:
\begin{itemize}
    \item $L^2$ Parameter Regularization (weight decay)
    \item $L^1$ Parameter Regularization
    \item Early Stopping
    \item Drop-Out
\end{itemize}

\subsubsection{Drop-Out}

Switching off a given ratio of random neurons before each training step helps to generalize better. The output nodes are never switched off. A typical dropout rate is 0.8 for the input layer and 0.5 for the hidden layers.

\newpar{}
\ptitle{Weight scaling}

Because only a part of the neurons are used during training the weights need to be scaled either during training or for the final inference.

% Let's assume we drop $0.5$ of the weights. We now either have to scale the remaining weights by $2$ \textbf{during} the training or all of the weights by $0.5$\textbf{after} training.

\newpar{}
Dropout is computationally cheap but it reduces the effective capacity of the model (variance).

\subsection{Convolutional Neural Networks (CNN)}

CNNs are useful to process images and other unstructured data. They attempt to learn the key image properties
\begin{itemize}
    \item local features
    \item features invariant to translation
    \item hierarchy (objects consist of multiple features)
    \item features equivariant: Same property of same object type yields same result
\end{itemize}

\newpar{}
\ptitle{Properties of CNNs}
\begin{itemize}
    \item [+] \textbf{Learn} features (instead of ``manual'' detection (e.g.\ Harris corner))
    \item [+] weight-sharing among images
\end{itemize}

\subsubsection{Architecture}
\begin{center}
    \includegraphics[width=\linewidth]{nn_CNN.png}
\end{center}
\newpar{}
\ptitle{Remarks}
\begin{itemize}
    \item The CNN kernels are found by backpropagation.
    \item The output shape shrinks with increasing depth, the number of channels increases.
\end{itemize}

% The convolution with a kernel can be viewed as a multiplication by a matrix like in the general feedforward NN case but with the difference that the weight matrix is a sparse block circular matrix. In other words, it does not represent a fully connected network but rather a network that only connects ``neighboring'' neurons and they shares the weights. 
% Convolution is thus dramatically more efficient than dense matrix multiplication in terms of memory requirements and statistical efficiency.

\subsubsection{Convolution}
The convolution for a kernel with an \textbf{odd} size is given by
\begin{equation*}
    (I * K)(i,j) = \sum_{m=-w_y}^{w_y}\sum_{n=-w_x}^{w_x}I(i+m,j+n)K(i,j)
\end{equation*}
with
\begin{equation*}
    w_x = \left\lfloor \frac{\dim_x(K)}{2} \right\rfloor \qquad
    w_y = \left\lfloor \frac{\dim_y(K)}{2} \right\rfloor
\end{equation*}

\newpar{}
\ptitle{Remarks}
\begin{itemize}
    \item Mathematically speaking this is a cross-correlation.
    \item The output size drops by $2\cdot (a//2)$ (floor division) for kernel size $a$.
          \begin{equation*}
              w_{out}=  w-2(a//2) \qquad
              h_{out}=  h-2(a//2)
          \end{equation*}
    \item [+] Parameter-efficient, local input, invariant under translation, equivariant
\end{itemize}

\paragraph{Convolution for Multi-Channel Inputs}
\ptitle{Multi-Filter Multi-Channel CNN Example}
\begin{center}
    \includegraphics[width=\linewidth]{nn_multi_channel_multi_filter_kernels.png}
\end{center}

\subsubsection{Padding}

For an input matrix of size $n\times n$ and a kernel of size $a\times a$

\newpar{}
\ptitle{Valid Padding}

No padding is added and the output size therefore is $n-a+1$

\newpar{}
\ptitle{Same Padding}

The input is padded with $p=(a-1)/2$ elements which preserves the input size. Normally zero-padding is used but in some cases other values are desired.

\newpar{}
\ptitle{Output Size}

Padding with width $p$ yields output size
\begin{equation*}
    w_{out}=  w+2p-2(a//2) \qquad
    h_{out}=  h+2p-2(a//2)
\end{equation*}

\subsubsection{Stride}
Stride is the step size of the moving convolution kernel.

Using padding $p$ and stride $s$ with a kernel of size $a$ one gets output size
\begin{equation*}
    w_{out}= (w+2p-a)//s + 1 \qquad
    h_{out}= (h+2p-a)//s + 1
\end{equation*}


\subsubsection{Pooling}
To get a compact representation the following pooling methods are used.
\begin{itemize}
    \item Max pooling (most common)
          \\ location of the max() element must be stored for backpropagation or un-pooling
    \item Average pooling
    \item Weighted average pooling
    \item $L^2$ norm pooling
\end{itemize}

Pooling helps to make the representation become approximately invariant to small translations of the input.

Pooling is also used to handle a varying size of the input.

\newpar{}
\ptitle{Un-Pooling}

The once pooled values can also be restored to the original size (with zeros in where needed). This helps to reduce feature map size. Locations of ``chosen'' pixels must be memorized.

\subsubsection{Image Segmentation}

\ptitle{Quality Metric: Intersection over Union} (IoU)
\begin{equation*}
    \text{IoU} = \frac{\text{Area of Intersection}}{\text{Area of Union}}
\end{equation*}

% TODO: should we add this?
% \subsubsection{U-net}

\subsection{RNN}\label{sec:RNN}
Recurrent Neural Networks use internal feedback of past neuron states (recurrent neurons) to improve performance on sequences of data.

\newpar{}
\ptitle{Feedback (FB) Types}

One distinguishes direct $W_d$, indirect $W_i$ and lateral $W_l$ FB
\begin{center}
    \includegraphics[width=0.5\linewidth]{nn_feedback_types.png}
\end{center}

\subsubsection{Sequences of Words}
To train RNN sequences of words are used instead of single words.

\ptitle{n-Grams}
n-grams are $n$ consecutive words/tokens in a text.
\begin{itemize}
    \item \textbf{Unigrams} treat each word independently
    \item \textbf{Bigram}, \textbf{trigrams} etc.\ treat 2,3,$\ldots$ words in combination
\end{itemize}
\textbf{Remarks}
\begin{itemize}
    \item The combination of words grows with $\mathcal{O}(c^n)$
    \item An \textbf{n-gram language model} predicts the probability of word $n$ given the words $0,\dots,n-1$. Two of the main issues are:
          \begin{itemize}
              \item Sparsity: sentences that are rare in the corpus have $P=0$ to be predicted. For sentences not given in the corpus one can't calculate $P$ at all.
              \item Storage: if the corpus is large storing all the possbile word permutations becomes hard.
          \end{itemize}
\end{itemize}

\subsubsection{Vanilla RNN}

\begin{itemize}
    \item[+] can process inputs of any length and the model size is independent of the length
    \item[+] use information of many past time steps
    \item[+] small number of shared parameters
    \item[-] training needs to be done sequentally - no parallelization
    \item[-] exploding and vanishing gradients during backpropagation. In case of vanishing gradients, past information gets lost.
\end{itemize}

\newpar{}
\ptitle{Direct Feedback}
\begin{center}
    \includegraphics[width=0.75\linewidth]{RNN.png}
\end{center}
The output $y_{(t)}$ depends on all n past inputs $x_{(t,\ldots,t-n)}$:
\noindent\begin{equation*}
    y_{(t)} = \sigma\Bigl(\mathbf{W}_x^{\mathsf{T}}\mathbf{x}_{(t)} + \sigma\bigl(\mathbf{W}_x^{\mathsf{T}}\mathbf{x}_{(t-1)} + \sigma( \ldots ) + \mathbf{b}\bigr) + \mathbf{b}\Bigr)
\end{equation*}

formulated differently
\noindent\begin{align*}
    \mathbf{h}_t & = \sigma(\mathbf{W}_\mathbf{x} \mathbf{x}_t + \mathbf{W}_\mathbf{h} \mathbf{h}_{t-1}) \\
    \mathbf{y}_t & = \mathrm{softmax}(\mathbf{W}_s \mathbf{h}_t)
\end{align*}

\subsubsection{LSTM}
Similar to RNNs, Long-Short Time Memory networks add saved past states for later use. However, they separate \textbf{long and short term} memory.

The long term state $C_{(t)}$ depends on the input $\mathbf{x}_{(t)}$, the long term memory $C_{(t-1)}$ and the previous output/hidden state $h_{(t-1)}$:
\begin{center}
    \includegraphics[width=0.5\linewidth]{LSTM.jpeg}
\end{center}

\textbf{Example}
Depending on the weights, the input $\mathbf{x}_{(t)}$ could have only a small effect on $y_{(t)}$ but a large effect on the long term memory $C_{(t)}$ and thus also on later outputs $y_{(t+n)} (n>1)$.

\newpar{}
\ptitle{Remarks}
\begin{itemize}
    \item [+] LSTM models solve the problem of vanishing gradients
    \item [+] easier to preserve information of many time steps (cell learns when to remember $h$ and when to forget it)
    \item [+] adaptive use of hidden states
    \item [-] slower than RNNs
    \item [-] over many time steps, information gets lost (solved by attention)
\end{itemize}
