\section{Ensembles}
An ensemble of `similar' classification or regression models can be combined to reduce the variance and/or bias by combining their individual results.

\subsection{Random Forests}
By combining the predicion results of $M$ classification trees with variance $\sigma^2$, the variance can be reduced to $\frac{\sigma^2}{M}$.

\subsubsection{Bootstrap Aggregation - Bagging}
To ensure that the deterministic trees generate different outputs, a \textbf{bootstrap dataset} ($N$ random datapoints with replacement sampled $M$ times) is used.

\newpar{}
The $m=1,\ldots, M$ results are \textbf{aggregated} depending on the type of problem:
\begin{itemize}
    \item \textbf{Hard Classification}:
          \begin{itemize}
              \item Tree $T_m$ yields \textbf{label estimate} $\widehat{c}_m(\mathbf{x})$
              \item Aggregate with \textbf{majority vote}:
                    \noindent\begin{equation*}
                        \widehat{c}_{RF} = \mathrm{mode}(\widehat{c}_1,\ldots, \widehat{c}_m)
                    \end{equation*}
          \end{itemize}
    \item \textbf{Probabilistic Classification}:
    \item \begin{itemize}
              \item Tree $T_m$ yields \textbf{class likelyhood} $\widehat{P}_m(c|\mathbf{x})$
              \item Aggregate with \textbf{mean distribution}:
                    \noindent\begin{equation*}
                        \widehat{P}_{RF} = \frac{1}{M}\sum_{m=1}^{M}\widehat{P}_m(C|\mathbf{x})
                    \end{equation*}
          \end{itemize}
    \item \textbf{Regression}:
          \begin{itemize}
              \item Tree $T_m$ yields \textbf{regression function} $\widehat{f}_m(\mathbf{x})$
              \item Aggregate with \textbf{averaging}:
                    \noindent\begin{equation*}
                        \widehat{f}_{RF} = \frac{1}{M}\sum_{m=1}^{M}\widehat{f}_m(\mathbf{x})
                    \end{equation*}
          \end{itemize}
\end{itemize}

As a result, if the errors of the individual models $\epsilon_m$ are \textbf{uncorrelated} and have \textbf{zero mean}, the error of the ensemble $E_{ens}$ is given by
\noindent\begin{equation*}
    E_{ens} = \frac{1}{M^2}\mathbb{E}_{\mathbf{x}} \left[\sum_{m=1}^{M} {\epsilon_m(\mathbf{x})}^2\right] = \frac{1}{M} E_{avg}
\end{equation*}

\textbf{Remark}

Trees are trained independent of each other and can therefor be trained in parallel.

\subsection{Boosting}
In boosting, the models are trained sequentially and after each step the data points which have be \textbf{misclassified} by the last iteration are given additional weight.
As a result, the subsequent training step `corrects' the error of the previous.

\newpar{}
\ptitle{\code{AdaBoost(Dataset $D$, targets $t_n\in\{-1,1\}$)}}
\fncode{
    \begin{algorithmic}
        \setstretch{1.5}
        \State{Initialize weights: ${w_{n}}^{(1)}\gets \frac{1}{N}$}
        \For{$m = 1, \ldots, M $}
        \State{$\mathbb{I} = \begin{cases}
                    1 & \widehat{y}_m(\mathbf{x}_n)\neq t_n \\
                    0 & else
                \end{cases}$}
        \State{$J_m\gets \sum_{n=1}^{N}w_n^{(m)} \mathbb{I}$}
        \State{fit model that minimizes $J_m$}
        \State{$\epsilon_m \gets \frac{\sum_{n}{w_n}^{(m)} \mathbb{I}}{\sum_{n}w_n^{(m)}} $}
        \State{$\alpha_m \gets \ln\left(\frac{1-\epsilon_m}{\epsilon_m}\right)$}
        \State{Update weights: $w_n^{(m+1)} \gets w_n^m\exp\left(\alpha_m \mathbb{I}\right)$}
        \EndFor{}
        \State{Predict using final model: $\widehat{Y}_M(\mathbf{x})\gets \mathrm{sign}{\sum_{m=1}^{M}\alpha_m\widehat{y}_m(\mathbf{x})}$}
    \end{algorithmic}
}

\subsection{Mixture of Experts}
Similar to bagging, multiple \textbf{different types of models} with different ``domains of expertise'' can be used to improve predictions.