\section{Ensembles}
An ensemble of `similar' classification or regression models can be combined to reduce the variance and/or bias by combining their individual results.

\subsection{Random Forests}
By combining predicions of $M$ independent classification trees with variance $\sigma^2$, overall variance can be reduced to $\frac{\sigma^2}{M}$.

\subsubsection{Bootstrap Aggregation - Bagging}
To ensure that the deterministic trees generate different outputs, a \textbf{bootstrap dataset} ($N$ random datapoints with replacement sampled $M$ times) is used.

\newpar{}
The $m=1,\ldots, M$ results are \textbf{aggregated} depending on the type of problem:
\begin{itemize}
    \item \textbf{Hard Classification}:
          \begin{itemize}
              \item Tree $T_m$ yields \textbf{label estimate} $\widehat{c}_m(\mathbf{x})$
              \item Aggregate with \textbf{majority vote}:
                    \noindent\begin{equation*}
                        \widehat{c}_{RF} = \mathrm{mode}(\widehat{c}_1,\ldots, \widehat{c}_m)
                    \end{equation*}
          \end{itemize}
    \item \textbf{Probabilistic Classification}:
    \item \begin{itemize}
              \item Tree $T_m$ yields \textbf{class likelyhood} $\widehat{P}_m(c|\mathbf{x})$
              \item Aggregate with \textbf{mean distribution}:
                    \noindent\begin{equation*}
                        \widehat{P}_{RF} = \frac{1}{M}\sum_{m=1}^{M}\widehat{P}_m(C|\mathbf{x})
                    \end{equation*}
          \end{itemize}
    \item \textbf{Regression}:
          \begin{itemize}
              % TODO: Why is this called a "Tree"? Can trees yield regression functions? I think one just trains regression models and averages them which has nothing to do with a tree.
              \item Tree $T_m$ yields \textbf{regression function} $\widehat{f}_m(\mathbf{x})$
              \item Aggregate with \textbf{averaging}:
                    \noindent\begin{equation*}
                        \widehat{f}_{RF} = \frac{1}{M}\sum_{m=1}^{M}\widehat{f}_m(\mathbf{x})
                    \end{equation*}
          \end{itemize}
\end{itemize}

\textbf{Remarks}
\begin{itemize}
    \item Trees are trained independent of each other and can therefore be trained in parallel.
    \item The main goal of bagging is \textbf{reduce variance}.
\end{itemize}

\paragraph{Error of Ensemble}
Using $M$ models, their average MSE is given by $E_{avg}$.
Then, if the errors of the individual models $\epsilon_m$ are \textbf{uncorrelated} and have \textbf{zero mean}, the MSE of the ensemble $E_{ens}$ is given by
\noindent\begin{equation*}
    E_{ens} = \frac{1}{M^2}\mathbb{E}_{\mathbf{x}} \left[\sum_{m=1}^{M} {\epsilon_m(\mathbf{x})}^2\right] = \frac{1}{M} E_{avg}
\end{equation*}
i.e.\ it is lower than the expected average MSE of the individual models.

\subsection{Mixture of Experts}
Similar to bagging, multiple \textbf{different types of models} with different ``domains of expertise'' can be used to improve predictions.

\subsection{Boosting}
In boosting, the models are trained sequentially and after each step the data points which have been \textbf{misclassified} by the last iteration are given additional weight.
As a result, the subsequent training step `corrects' the error of the previous.

% TODO: We could comment it out for the exam if we need more space (not exam relevant).
% \newpar{}
% \ptitle{\code{AdaBoost(Dataset $D$, targets $t_n\in\{-1,1\}$)}}
% \fncode{
%     \begin{algorithmic}
%         \setstretch{1.5}
%         \State{Initialize weights: ${w_{n}}^{(1)}\gets \frac{1}{N}$}
%         \For{$m = 1, \ldots, M $}
%         \State{$\mathbb{I} = \begin{cases}
%                     1 & \widehat{y}_m(\mathbf{x}_n)\neq t_n  \quad \mathrm{misclassified}\\
%                     0 & else
%                 \end{cases}$}
%         \State{$J_m\gets \sum_{n=1}^{N}w_n^{(m)} \mathbb{I}$}
%         \State{fit model that minimizes $J_m$}
%         \State{$\epsilon_m \gets \frac{\sum_{n}{w_n}^{(m)} \mathbb{I}}{\sum_{n}w_n^{(m)}} $}
%         \State{$\alpha_m \gets \ln\left(\frac{1-\epsilon_m}{\epsilon_m}\right)$}
%         \State{Update weights: $w_n^{(m+1)} \gets w_n^m\exp\left(\alpha_m \mathbb{I}\right)$}
%         \EndFor{}
%         \State{Predict using final model: $\widehat{Y}_M(\mathbf{x})\gets \mathrm{sign}{\sum_{m=1}^{M}\alpha_m\widehat{y}_m(\mathbf{x})}$}
%     \end{algorithmic}
% }

\textbf{Remarks}
\begin{itemize}
    \item $J_m$ is the weighted error function.
    \item The main goal of boosting is to \textbf{reduce bias}.
    \item For Inference, run models in parallel and weight outputs with training weights.
\end{itemize}