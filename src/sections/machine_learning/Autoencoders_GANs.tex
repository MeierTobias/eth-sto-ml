\section{Autoencoders}
Autoencoders can be used for various tasks. Some popular examples are
\begin{itemize}
    \item Dimensionality reduction
    \item Generative tasks
    \item Denoising (add noise to training vector but not to target)
    \item Text generation
\end{itemize}
The general structure of an autoencoder looks the following:
\begin{center}
    \includegraphics[width=0.8\linewidth]{autoencoders_structure.png}
\end{center}
The latent (intermediate) representation $\mathbf{z}$ is denser than $\mathbf{x}$. The reconstruction loss is given by
\begin{equation*}
    \left\|\mathbf{x}-\hat{\mathbf{x}}\right\|_2=\left\|\mathbf{x}-\mathbf{d_\phi}(\mathbf{z})\right\|_2=\left\|\mathbf{x}-\mathbf{d_\phi}(\mathbf{e_\theta}(\mathbf{x}))\right\|_2
\end{equation*}

\newpar{}
\ptitle{Number of Latent Features}

The number of latent features is a hyperparameter and hence task-dependent.
\begin{center}
    \includegraphics[width=\linewidth]{autoencoders_latent.png}
\end{center}

\subsection{Autoencoders for Generative Tasks}
One inputs a random sample and uses the \textbf{decoder} go get a reasonable random output sample.
% \begin{center}
%     \includegraphics[width=0.5\linewidth]{autoencoders_generation.png}
% \end{center}

\subsection{Autoencoders for Dimensionality Reduction}
The latent representation $\mathbf{z}$ is dense and can be seen as a nonlinear analogon of PCA.\ One uses \textbf{only the encoder} part.
% \begin{center}
%     \includegraphics[width=0.5\linewidth]{autoencoders_dim_reduction.png}
% \end{center}

\subsubsection{Autoencoders vs.\ PCA}
\begin{center}
    \includegraphics[width=\linewidth]{autoencoders_vs_pca.png}
\end{center}

\section{Generative Adversarial Networks (GANs)}

To generate synthetic data a GAN can be used. It consists of two parts, a generator which has a decoder structure and a discriminator. 
\begin{center}
    \includegraphics[width=0.9\linewidth]{GAN.png}
\end{center}
\begin{enumerate}
    \item The generator gets random noise as an input and created synthetic samples.
    \item These synthetic and the real samples are then fed into the discriminator which predicts if the given data is real or synthetic.
    \item Using backpropagation either the discriminator or the generator weights can be adjusted.
\end{enumerate}
% The goal is to generate synthetic data that can not be distinguished from real data. The so trained generator can then be used for other tasks that require synthetic data.