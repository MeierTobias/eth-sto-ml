\section{Autoencoders}
Autoencoders can be used for various tasks. Some popular examples are 
\begin{itemize}
    \item Dimensionality reduction
    \item Generative tasks
    \item Denoising (add noise to training vector but not to target)
    \item Text generation
\end{itemize}
The general structure of an autoencoder looks the following:
\begin{center}
    \includegraphics[width=\linewidth]{autoencoders_structure.png}
\end{center}
The latent (intermediate) representation $\mathbf{z}$ is denser than $\mathbf{x}$. The reconstruction loss is given by
\begin{equation*}
    \left\|\mathbf{x}-\hat{\mathbf{x}}\right\|_2=\left\|\mathbf{x}-\mathbf{d_\phi}(\mathbf{z})\right\|_2=\left\|\mathbf{x}-\mathbf{d_\phi}(\mathbf{e_\theta}(\mathbf{x}))\right\|_2
\end{equation*}

\newpar{}
\ptitle{Train with Noise}

As the optimal encoder-decoder autoencoder could just learn to multiply the input by $1$ one can add noise to the input and make the autoencoder learn to reconstruct the original vector. This way, the autoencoder just extracts the most relevant features.

\newpar{}
\ptitle{Number of Latent Features}

The number of latent features is a hyperparameter and hence task-dependent.
\begin{center}
    \includegraphics[width=0.75\linewidth]{autoencoders_latent.png}
\end{center}

\subsection{Autoencoders for Dimensionality Reduction}
The latent representation $\mathbf{z}$ is dense and can be seen as a nonlinear analogon of PCA.\ One uses the encoder part.
\begin{center}
    \includegraphics[width=0.6\linewidth]{autoencoders_dim_reduction.png}
\end{center}

\paragraph{Autoencoders vs.\ PCA}
\begin{center}
    \includegraphics[width=\linewidth]{autoencoders_vs_pca.png}
\end{center}

\subsection{Autoencoders for Generative Tasks}
One inputs a random sample and uses the decoder go get a reasonable random output sample.
\begin{center}
    \includegraphics[width=0.6\linewidth]{autoencoders_generation.png}
\end{center}

\section{Generative Adversarial Networks (GANs)}
% TBD




