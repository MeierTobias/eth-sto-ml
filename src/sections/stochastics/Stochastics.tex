\section{Sample Spaces and Probability Measures}
\subsection{Notation}
\noindent\begin{align*}
     & \omega                 &  & \text{Possible outcome}                               \\
     & \Omega                 &  & \text{Sample space}                                   \\
     & A \subseteq \Omega     &  & \text{Event (logical collection of outcomes)}         \\
     & {\{x_1 \dots x_n \}}^k &  & \text{all sequences of length k using elements } x_i. \\
     & \mathbb{P}(A)          &  & \text{Probability that A occurs.}
\end{align*}
\subsection{De Morgan's Laws}
\noindent\begin{align*}
    {(A\cup B)}^C = A^C\cap B^C \\
    {(A\cap B)}^C = A^C\cup B^C
\end{align*}

\subsection{Axioms of Probability Theory}
\begin{enumerate}
    \item $0\leq \mathbb{P}(A)\leq 1$
    \item $\mathbb{P}(\Omega)$ = 1
    \item $\mathbb{P}\left(\cup_{i\geq 1} A_i\right) = \mathbb{P}\underbrace{(A_1 \cup A_2 \cup \dots)}_{\text{countably infinite}} = \sum_{i\geq 1} \mathbb{P}(A_i)$\\
          if $A_{i} \cap A_{j} = \emptyset \; \forall i \ne j$ (piecewise disjoint)
\end{enumerate}
A sample space $\Omega$ with a probability measure $\mathbb{P}$ forms a \textbf{probability space}.

\subsubsection{Further rules from the Axioms}\label{sssec:rules_from_axioms}
\noindent\begin{align*}
     & \mathbb{P}(\emptyset) = 0                                                                                                                                  \\
     & \mathbb{P}  \underbrace{(A_1 \cup \dots \cup A_n)}_{\text{finite}} = \sum_{i=1}^{n} \mathbb{P}(A_i) & \text{if } A_i\cap A_j = \emptyset\, \forall i\neq j \\
     & \mathbb{P}(A^C) = 1-\mathbb{P}(A)                                                                                                                          \\
     & \mathbb{P}(A\cup B) = \mathbb{P}(A)+\mathbb{P}(B) - \mathbb{P}(A\cap B)                                                                                    \\
     & \mathbb{P}(A_1 \cup \dots \cup A_n) \leq \mathbb{P}(A_1)+\dots \mathbb{P}(A_n)                                                                             \\
     & \mathbb{P}(B) \leq \mathbb{P}(A)                                                                    & \text{if } B\subseteq A                              \\
     & \mathbb{P}(A\backslash B) = \mathbb{P}(A)-\mathbb{P}(B)                                             & \text{if } B\subseteq A
\end{align*}

\ptitle{Useful Tricks}
\noindent\begin{align*}
    D                   & =(D\cap C) \cup (D\cap C^C)                   &  & C\cap C^C \overset{\text{disjoint}}{=} \emptyset \\
    \mathbb{P}(D)       & = \mathbb{P}(D\cap C) + \mathbb{P}(D\cap C^C)                                                       \\\\
    D\cap C^C           & = D\backslash C                               &  & \text{dependent}                                 \\
    \mathbb{P}(C\cup D) & = \mathbb{P}(C) + \mathbb{P}(D\backslash C)   &  & \text{dependent}
\end{align*}

\subsection{Discrete Probability Spaces}
A discrete probability space has at most countably many different elements. As the outcomes exclude each other:
\noindent\begin{align*}
    \mathbb{P}(A) & = \mathbb{P}\left(\bigcup_{\substack{\omega_i \in A}}\{\omega_i\}\right)= \sum_{\substack{\omega_i \in A}} \mathbb{P}(\omega_i)
\end{align*}
\subsubsection{Laplace Model}
In the Laplace model, all possible outcomes have the same probability.
\noindent\begin{align*}
    \mathbb{P}(\omega_i) & = \frac{1}{|\Omega|}                                             \\
    \mathbb{P}(A)        & = \sum_{\omega_i \in A}\frac{1}{|\Omega|} = \frac{|A|}{|\Omega|}
\end{align*}

\section{Independence, Conditional Prob.\ and Bayes}
\subsection{Independence}
\ptitle{General}

\noindent\begin{gather*}
    A \cap B = \emptyset \Rightarrow \mathbb{P}(A\cap B) = 0 \\
    B \subseteq A \Rightarrow \mathbb{P}(A\cap B) = \mathbb{P}(B) \\
    0 \leq \mathbb{P}(A\cap B) \leq \min\{\mathbb{P}(A), \mathbb{P}(B)\}
\end{gather*}
and therefore also
\begin{equation*}
    \max\{0, 1-\mathbb{P}(A^C) -\mathbb{P}(B^C)\} \leq \mathbb{P}(A\cap B) \leq \min\{ \mathbb{P}(A), \mathbb{P}(B)\}
\end{equation*}
because
\begin{equation*}
    P(A\cap B)=1-\underbrace{P(A^C \cup B^C)}_{\le P(A^C)+P(B^C)}\ge 1-(P(A^C)+P(B^C))
\end{equation*}

\ptitle{Independent}

$A$ and $B$ are independent if
\noindent\begin{equation*}
    \mathbb{P}(A\cap B) = \mathbb{P}(A)\mathbb{P}(B)
\end{equation*}
For several events $A_{1\dots n}$ and each choice $A_{i_1\dots i_k}$ with $k\leq n$
\noindent\begin{equation*}
    \mathbb{P}(A_{i_j} \cap \dots \cap A_{i_k}) = \mathbb{P}(A_{i_j}) \cdots \mathbb{P}(A_{i_k})
\end{equation*}
holds. This implies that each subset of events must be independent as well. \textbf{Caution}: The opposite doesn't hold i.e.\ if one has independent subsets this doesn't mean that the total intersection of the subsets is independent.

\ptitle{Independence of Complement}

\noindent\begin{equation*}
    \mathbb{P}(A\cap B) = \mathbb{P}(A)\mathbb{P}(B) \Leftrightarrow \mathbb{P}(A\cap B^C) = \mathbb{P}(A)\mathbb{P}(B^C)
\end{equation*}

\subsection{Total Probability}
\noindent\begin{align*}
    \mathbb{P}(A)     & =\mathbb{P}(A\cap B)+\mathbb{P}(A\cap B^c)                                           \\
                      & =\mathbb{P}(A\mid B)\mathbb{P}(B)+\mathbb{P}(A\mid B^c)\mathbb{P}(B^c)               \\\\
    \mathbb{P}(A)     & =\sum_{i=1}^k\mathbb{P}(A\cap B_i) =\sum_{i=1}^k\mathbb{P}(A\mid B_i)\mathbb{P}(B_i) \\
    \text{if } \Omega & =B_1\cup\cdots\cup B_k,\quad\text{with }B_i\cap B_j=\emptyset\text{ for }i\neq j
\end{align*}
\subsection{Conditional Prob.\ and Bayes}
\noindent\begin{align*}
    \mathbb{P}(B\mid A) & =\frac{\mathbb{P}(A\cap B)}{\mathbb{P}(A)}=\frac{\mathbb{P}(A\mid B)\mathbb{P}(B)}{\mathbb{P}(A)} \overset{\text{indep.}}{=} \mathbb{P}(B)
\end{align*}
If $\Omega=B_1\cup\cdots\cup B_k\quad$ with $B_i\cap B_j=\emptyset$ for $i\neq j$, then by the \textit{law of total probability}:
\noindent\begin{align*}
    \mathbb{P}(B_i\mid A) & =\frac{\mathbb{P}(A\cap B_i)}{\mathbb{P}(A)}=\frac{\mathbb{P}(A\mid B_i)\mathbb{P}(B_i)}{\mathbb{P}(A)}                              \\
                          & =\frac{\mathbb{P}(A\mid B_i)\mathbb{P}(B_i)}{\sum_{j=1}^k \underbrace{\mathbb{P}(A\mid B_j)\mathbb{P}(B_j)}_{\mathbb{P}(A\cap B_j)}}
\end{align*}

\textbf{Remark:} The conditional probability $\mathbb{P}(\cdot|B)$ can be viewed as a new probability measure over $\Omega=B$. Thus, the usual probability rules stated in Subsubsection\ \ref{sssec:rules_from_axioms} hold e.g.\:
\begin{align*}
    \mathbb{P}(A_1\cup A_2\mid B) & =\mathbb{P}(A_1\mid B)+\mathbb{P}(A_2\mid B)\quad\text{for}\quad A_1\cap A_2=\emptyset \\
    \mathbb{P}(A^c\mid B)         & =1-\mathbb{P}(A\mid B)
\end{align*}
but in general
\begin{equation*}
    \mathbb{P}(A\mid B^c)\neq1-\mathbb{P}(A\mid B)
\end{equation*}

\section{Random Variables}
When using random variables, the event $A$ is a subset of $\mathbb{R}$, and its probability given by
\noindent\begin{align*}
    X : \Omega         & \rightarrow \mathbb{R}                                                                       \\
    \mathbb{P}(X\in A) & =\mathbb{P}(\underbrace{\{\omega\in\Omega:X(\omega)\in A\}}_{=X^{-1}(A)\text{ := preimage}}) % ChkTex 26
\end{align*}
Thus, events can be defined using intervals:
\noindent\begin{align*}
    \mathbb{P}(a\leq X\leq b) & =\mathbb{P}(X\in[a,b])
\end{align*}

\subsection{Discrete Random Variables}
\noindent\begin{equation*}
    X:\Omega\rightarrow W=\{x_1,x_2,x_3,\ldots\}\subseteq\mathbb{R}
\end{equation*}
A random variable $X$ is said to be \textit{discrete} if its \textit{range} $W$ is \textbf{at most countable}.
\newpar{}
\ptitle{PMF}

The \textit{probability mass function (pmf)} of $X$ is given by
\begin{equation*}
    p(x_{k}) =\mathbb{P}(X=x_{k})
\end{equation*}
For the pmf one then has the properties
\noindent\begin{align*}
    \mathbb{P}(X\in A)  & =\sum_{k:x_k\in A}p(x_k)                        \\
    \sum_{k\geq1}p(x_k) & =1                       & \text{Normalization}
\end{align*}

\newpar{}
\ptitle{Distribution}

The \textit{distribution} of $X$ describes probabilities for subsets (e.g.\ intervals) on the real line. It is the probability measure $\mathbb{Q}$ on $\mathbb{R}$ given by
\noindent\begin{equation*}
    \mathbb{Q}(A)=\mathbb{P}(X\in A)=\sum_{k:x_k\in A} \underbrace{p(x_k)}_{\text{pmf}}\quad\mathrm{for~}A\subseteq\mathbb{R}
\end{equation*}

\newpar{}
\ptitle{CDF}

The \textit{cumulative distribution function (cdf)} of $X$ is the function
\noindent\begin{align*}
    F:\mathbb{R}            & \rightarrow [0,1]                             \\
    F(x)=\mathbb P(X\leq x) & =\sum_{k:x_k\leq x}p(x_k),\quad x\in\mathbb R
\end{align*}

\subsubsection{Properties of CDFs}
\begin{itemize}
    \item F is non-decreasing (weakly increasing)
    \item $\lim_{x\rightarrow -\infty}F(x)=0$ and $\lim_{x\rightarrow \infty}F(x)=1$
    \item F is right-continuous.\ i.e. $\lim_{y\downarrow x} F(y) =F(x)$ (i.e.\ $F(y)$ converges towards $F(x)$ when $y>x$ converges towards $x$)
\end{itemize}

\subsubsection{General Rules for Discrete Random Variables}
\noindent\begin{align*}
    \mathbb{P}(X>x)       & =1-\mathbb{P}(X\leq x)=1-F(x)                                   \\
    \mathbb{P}(X\geq x)   & =\mathbb{P}(X>x)+\mathbb{P}(X=x)=1-F(x)+p(x)                    \\
    \mathbb{P}(a<X\leq b) & =\mathbb{P}(X\leq b)-\mathbb{P}(X\leq a)=F(b)-F(a)              \\
    p(x_k)                & \overset{\text{adjacent in } W}{=}\mathbb{P}(x_{k-1}<X\leq x_k) \\
                          & =F(x_k)-F(x_{k-1})
\end{align*}

\subsection{Independence of Random Variables}
Two random variables $X,Y_\Omega \rightarrow\mathbb{R}$ are \textbf{independent} if
\noindent\begin{align*}
    \mathbb{P}(X\in A,Y\in B)            & =                                                                        \\
    % is the overset{!}{=} below really necessary?
    %\mathbb{P}(\{X\in A\}\cap\{Y\in B\}) & \overset{!}{=}\mathbb{P}(X\in A)\mathbb{P}(Y\in B)\quad\forall A,B\subseteq\mathbb{R}
    \mathbb{P}(\{X\in A\}\cap\{Y\in B\}) & =\mathbb{P}(X\in A)\mathbb{P}(Y\in B)\quad\forall A,B\subseteq\mathbb{R}
\end{align*}

Several random variables $X_a, \dots X_n :\Omega\rightarrow\mathbb{R}$ are \textbf{independent} if
\noindent\begin{align*}
    \mathbb{P}(X_1\in A_1,\ldots,X_n\in A_n) = & \mathbb{P}(X_1\in A_1)\cdots\mathbb{P}(X_n\in A_n) \\
                                               & \forall A_1,\ldots,A_n\subseteq\mathbb{R}
\end{align*}
% TBD: Again, all subsets of events must be independent, right?

\subsection{Statistical Moments}
Statistical moments are important summary figures to describe distributions but do not determine the whole distribution. For unique representation $p$ and $F$ are required.
\subsubsection{Expectation}
The \textbf{expectation} of a discrete random variable $X$
\noindent\begin{equation*}
    \mu_{X}=\mathbb{E}[X]=\sum_{k\geq1}x_{k}p(x_{k})\in\mathbb{R}
\end{equation*}
can be interpreted as the
\begin{itemize}
    \item mean location of the distribution
    \item average under a large number of repetitions
    \item center of mass of the pmf.
\end{itemize}

\newpar{}
% TBD: I guess, we can again use the 2nd formula "E[g^2] - E[g]^2" too?
If $Y=g(X)$ is a \textbf{transformation} of the random variable $X$, its \textbf{expectation value} is given by
\noindent\begin{align*}
    \mathbb{E}[Y]    & =\mathbb{E}[g(X)]=\sum_{k\geq1}g(x_{k})p(x_{k})                    \\
    \mathbb{E}[g(X)] & \neq g(\mathbb{E}[X])                           & \text{generally}
\end{align*}

\subsubsection{Properties of the Expectation}
These properties also hold for non-discrete random variables if $\mathbb{E}[X]$ and $\mathbb{E}[Y]$ are well-defined.
\noindent\begin{align*}
    \textbf{Constants:}                   &  &  & a\in\mathbb{R},\mathbb{E}[a]=a\cdot p(a)=a                                          \\
    \textbf{Linearity:}                   &  &  & a\mathbb{E}[X]=\mathbb{E}[aX],\;\;a\in \mathbb{R}                                   \\
                                          &  &  & \mathbb{E}[X]+\mathbb{E}[Y]=\mathbb{E}[Z],\;\; Z=X+Y                                \\
    \textbf{Monotonicity:}                &  &  & \mathbb{E}[X]=\sum_{k\geq1}x_k\mathbb{P}(X=x_k)\geq0, \; W_x \subseteq \mathbb{R}_+ \\
    X(\omega)\geq Y(\omega)\forall\omega: &  &  & \operatorname{E}[X]-\operatorname{E}[Y]=\operatorname{E}[X-Y]\geq0                  \\\\
    \text{Assuming}                       &  &  & \text{independence of } X,Y:                                                        \\
    \textbf{Product:}                     &  &  & \mathbb{E}[X]\mathbb{E}[Y] = \mathbb{E}[XY] = \mathbb{E}[Z],                        \\
                                          &  &  & Z=XY, \;W_Z =\{x_1y_1,x_1y_2,\ldots,x_2y_1,\ldots\}                                 \\
                                          &  &  & \text{which can be extended to }X_1,\ldots,X_n
\end{align*}

\subsubsection{Variance}
The \textbf{variance} of a discrete random variable
\noindent\begin{align*}
    \mathrm{Var}(X) & = \mathbb{E}[{(X-\mathbb{E}[X])}^2] = \mathbb{E}[{(g(X)-\mathbb{E}[g(X)])}^{2}] \\
                    & = \sum_{k\geq1}{(x_k-\mu_X)}^2p(x_k)\geq0
\end{align*}
can be interpreted as the
\begin{itemize}
    \item mean quadratic deviation from the mean
    \item measure of dispersion
\end{itemize}

\newpar{}
If $Y=g(X)$ is a \textbf{transformation} of the random variable $X$, its \textbf{variance} is given by
\noindent\begin{align*}
    \mathrm{Var}(g(X)) & =\mathbb{E}[{(g(X)-\mathbb{E}[g(X)])}^{2}]                                 \\
                       & =\sum_{k\geq1}{(g(x_{k})-\mathbb{E}[g(X)])}^{2}p(x_{k})                    \\
    \mathrm{Var}(g(X)) & \neq g(\mathrm{Var}(X))                                 & \text{generally}
\end{align*}

\subsubsection{Standard Deviation}
The \textbf{standard deviation} is the square root of the \textbf{variance}
\noindent\begin{equation*}
    \sigma_{X}=\sqrt{\mathrm{Var}(X)}
\end{equation*}
and has the same unit as $X$.

\subsubsection{Properties of Variance and Standard Deviation}
\ptitle{Variance}

The variance is \textbf{not linear}. (Assuming all expectations are well defined)
\noindent\begin{align*}
    \mathrm{Var}(X)      & =\mathbb{E}[{(X-\mathbb{E}[X])}^2] = \mathbb{E}[X^2]-{\mathbb{E}[X]}^2 \\
    \mathrm{Var}(X)\geq0 & \Rightarrow\mathbb{E}[X^{2}]\geq{\mathbb{E}[X]}^{2}                    \\
    \mathrm{Var}(a)      & =\mathbb{E}[{(a-\mathbb{E}[a])}^2]=\mathbb{E}[{(a-a)}^2]=0             \\
    \mathrm{Var}(a+bX)   & = b^2\mathrm{Var}(X)\text{ (invariant under translation)}
\end{align*}
\ptitle{Variance of Sums}

\noindent\begin{align*}
    \mathrm{Var}(a_1X_1 & +\cdots+a_n X_n)  =\sum_{i=1}^{n}a_{i}^{2}\mathrm{Var}(X_{i})+\ldots                  \\
                        & + \underbrace{2\sum_{i<j}a_{i}a_{j}\mathrm{Cov}(X_{i},X_{j})}_{0\text{ if indp.}}     \\
    \mathrm{Var}(X+Y)   & =\mathrm{Var}(X)+\mathrm{Var}(Y)+ \underbrace{2\mathrm{Cov}(X,Y)}_{0\text{ if indp.}} \\
    \mathrm{Var}(X-Y)   & =\mathrm{Var}(X)+\mathrm{Var}(Y)-\underbrace{2\mathrm{Cov}(X,Y)}_{0\text{ if indp.}}
\end{align*}
Caution: $\mathrm{Var}(X\pm Y) = \mathrm{Var}(X)+\mathrm{Var}(Y)$ does not necessarily mean independence because $\mathrm{Cov}(X,Y)=0$ can also hold for perfect nonlinear dependence: \textbf{independence is much stronger} than uncorrelatedness.

\ptitle{Standard Deviation}
\noindent\begin{equation*}
    \sigma_{a+bX}=\sqrt{\mathrm{Var}(a+bX)}=\sqrt{b^2\mathrm{Var}(X)}=|b|\sigma_X
\end{equation*}

\subsubsection{Covariance and Correlation}
The \textbf{covariance} between two (discrete) random variables $X,Y$
\noindent\begin{align*}
    \operatorname{Cov}(X,Y) & =\mathbb{E}[(X-\mathbb{E}[X])(Y-\mathbb{E}[Y])] \\
                            & = \mathbb{E}[XY]-\mathbb{E}[X]\mathbb{E}[Y]
\end{align*}
\begin{itemize}
    \item is (only) a measure of \textbf{linear dependence} between $X$, $Y$
    \item can not tell wether $X$ causes $Y$, $Y$ causes $X$, or both are caused by a third variable $Z$
    \item if $\operatorname{Cov}(X,Y)$ and a certain $x$ are given we can conclude on where we expect $y$ to be with respect to $\mathbb{E}[Y]$ (and vice versa)
\end{itemize}

\newpar{}
The \textbf{correlation} between two (discrete) random variables $X,Y$
\noindent\begin{equation*}
    \mathrm{Corr}(X,Y)=\rho_{XY}=\frac{\mathrm{Cov}(X,Y)}{\sigma_{X}\sigma_{Y}}
\end{equation*}
\begin{itemize}
    \item is a \textbf{normalized} version of the covariance:\newline$-1 \leq \rho_{XY} \leq 1$
    \item \textbf{Perfect positive correlation}\newline $\rho_{XY}=1\Leftrightarrow Y=a+bX\mathrm{~for~}b>0$
    \item \textbf{Perfect negative correlation}\newline $\rho_{XY}=-1\Leftrightarrow Y=a+bX\mathrm{~for~}b<0$
    \item if e.g.\ $Y=\exp(X)$, one has perfect dependence but $\mathrm{Corr}(X,Y)<1$
\end{itemize}

Caution:
\begin{itemize}
    \item \textbf{If and only if} for perfect linear dependence one has $\rho_{XY}=1$.
    \item $\rho_{XY}<1$ (or even $\rho_{XY}=0$) only means that there is no perfect linear dependence but there could still be a perfect nonlinear dependence!
\end{itemize}

\subsubsection{Properties of the Covariance and Correlation}
(Assuming all expectations are well defined)
\noindent\begin{align*}
    \mathrm{Cov}(X,Y)        & =\mathbb{E}[(X-\mathbb{E}[X])(Y-\mathbb{E}[Y])]             \\
                             & = \mathbb{E}[XY]-\mathbb{E}[X]\mathbb{E}[Y]                 \\
    \mathrm{Cov}(X,X)        & = \mathrm{Var}(X)                                           \\
    \operatorname{Cov}(X,Y)  & = 0 \Rightarrow X,Y\text{ idp.}                             \\
    \mathrm{Cov}(a+bX,c+dY)  & =bd\operatorname{Cov}(X,Y),\quad b,d\in\mathbb{R}           \\
    \mathrm{Corr}(a+bX,c+dY) & ={\frac{bd}{|bd|}}\mathrm{Corr}(X,Y),\quad b,d\in\mathbb{R}
\end{align*}

\subsection{Discrete Distributions}

\subsubsection{Bernoulli Distribution}
$X \sim \mathrm{Ber}(p)$

\renewcommand{\arraystretch}{1.3}
\setlength{\oldtabcolsep}{\tabcolsep}\setlength\tabcolsep{3pt}

\begin{tabularx}{\linewidth}{@{}p{0.5\linewidth}p{0.49\linewidth}@{}}
    $W=\{0,1\}$                                     &
    \multirow{4}{*}{
        \input{plots/bernoulli_dist.tex}
    }                                                 \\
    $\mathbb{P}(X=0)=1-p,\newline\mathbb{P}(X=1)=p$ & \\
    $\mathbb{E}[X] = p$                             & \\
    $\mathrm{Var}(X) = p(1-p)$                      &
\end{tabularx}

\renewcommand{\arraystretch}{1}
\setlength\tabcolsep{\oldtabcolsep}



\subsubsection{Binomial Distribution}
$X \sim \mathrm{Bin}(n,p)$\\
$X$ = number of successes in $n$ independent Bernoulli experiments.

\renewcommand{\arraystretch}{1.3}
\setlength{\oldtabcolsep}{\tabcolsep}\setlength\tabcolsep{0pt}

\begin{tabularx}{\linewidth}{@{}p{0.5\linewidth}p{0.49\linewidth}@{}}
    $W=\{0,1,\ldots,n\}$                                                                                             &
    \multirow{4}{*}{
        \begin{minipage}{\linewidth}
            \input{plots/binomial_dist.tex}
        \end{minipage}
    }                                                                                                                  \\
    $\mathbb{P}(X=k)={n\choose k}p^k{(1-p)}^{n-k}\newline k\in {0,1,\ldots, n},\; {n\choose k}=\frac{n!}{k!(n-k)!} $ & \\
    $\mathbb{E}[X] = np$                                                                                             & \\
    $\mathrm{Var}(X) = np(1-p)$                                                                                      &
\end{tabularx}

\renewcommand{\arraystretch}{1}
\setlength\tabcolsep{\oldtabcolsep}

\subsubsection{Geometric Distribution}
$X \sim \mathrm{Geom}(p)$\\
$X$ = number of independent Bernoulli trials until first success.

\renewcommand{\arraystretch}{1.3}
\setlength{\oldtabcolsep}{\tabcolsep}\setlength\tabcolsep{3pt}

\begin{tabularx}{\linewidth}{@{}p{0.5\linewidth}p{0.49\linewidth}@{}}
    $W=\{1,2,\ldots\}$                                      &
    \multirow{4}{*}{
        \input{plots/geometric_dist.tex}
    }                                                         \\
    $\mathbb{P}(X=k)={(1-p)}^{k-1}p\newline k\in\mathbb{N}$ & \\
    $\mathbb{E}[X] = \frac{1}{p}$                           & \\
    $\mathrm{Var}(X) = \frac{1-p}{p^2}$
\end{tabularx}

\renewcommand{\arraystretch}{1}
\setlength\tabcolsep{\oldtabcolsep}

The geometric distribution for positive integers is \textbf{memoryless}.

\subsubsection{Poisson Distribution}
$X \sim \mathrm{Pois}(\lambda)$

\renewcommand{\arraystretch}{1.3}
\setlength{\oldtabcolsep}{\tabcolsep}\setlength\tabcolsep{3pt}
\begin{tabularx}{\linewidth}{@{}p{0.5\linewidth}p{0.49\linewidth}@{}}
    $W=\{0,1,2,\ldots\}$                                                        &
    \multirow{4}{*}{
        \input{plots/poisson_dist.tex}
    }                                                                             \\
    $\mathbb{P}(X=k)=e^{-\lambda}\frac{\lambda^k}{k!}\newline k\in\mathbb{N}_0$ & \\
    $\mathbb{E}[X] = \lambda$                                                   & \\
    $\mathbb{E}[X(X-1)] = \lambda^2$                                            & \\
    $\mathrm{Var}(X) = \lambda$                                                 &
\end{tabularx}
\renewcommand{\arraystretch}{1}
\setlength\tabcolsep{\oldtabcolsep}
\textbf{Remark:} For large $n$ and small $p$, $\mathrm{Bin}(n,p)\simeq \mathrm{Pois}(\lambda)$, $\lambda=np$.

For independent $X \sim \mathrm{Pois}(\lambda_1)$ and $Y \sim \mathrm{Pois}(\lambda_2)$ one has $X+Y \sim \mathrm{Pois}(\lambda_1 +\lambda_2)$

\subsection{Continuous Random Variables}
A random variable $X$ is continuous, if
\noindent\begin{equation*}
    F(x)=\mathbb{P}(X\leq x)
\end{equation*}
is continuous in $x$.


\subsubsection{Densities}
An integrable function $f:\mathbb{R}\rightarrow\mathbb{R}_+$ is a \textit{probability density} of $X$ if
\noindent\begin{equation*}
    \underbrace{F(x)}_{cdf} := \int_{-\infty}^{x} \underbrace{f(u)}_{pdf}\; du,\;\; \forall x\in \mathbb{R}
\end{equation*}
\begin{itemize}
    \item If $f$ is not continuous, $F$ is not differentiable everywhere
    \item If $F$ is differentiable, then $f = F'$
    \item Density $\Rightarrow$ continuous (opposite not given)
    \item In contrast to the discrete case, $f(x)>1$ is possible
    \item $f$ must be \textbf{normalized} and satisfy\newline $f(x)\geq 0 \; \forall x\in\mathbb{R}$
\end{itemize}
\noindent\begin{align*}
    \mathbb{P}(a\leq X\leq b)=\mathbb{P}(a<X\leq b)=F(b)-F(a)=\int_a^b f(u)du
\end{align*}
\subsubsection{Expectation and Variance}
\noindent\begin{align*}
    \mu_X=\mathbb{E}[X]                                 & =\int_{-\infty}^{\infty}xf(x)dx                               \\
    \mu_Y=\mathbb{E}[Y]                                 & =\int_{-\infty}^{\infty}g(x)f(x)dx,\;Y=g(X)                   \\
    \mathrm{Var}(X)=\mathbb{E}[{(X-\mu_{X})}^{2}]       & =\int_{-\infty}^{\infty}{(x-\mu_{X})}^{2}f(x)dx               \\
    \mathrm{Var}(Y)=\operatorname{E}[{(Y-\mu_{Y})}^{2}] & =\int_{-\infty}^{\infty}{\left(g(x)-\mu_{Y}\right)}^{2}f(x)dx
\end{align*}
\ptitle{Properties of Expectation and Variance}\\
The same properties as for discrete random variables hold, for example:
\noindent\begin{align*}
    \mathbb{E}[a+bX+cY]               & =a+b\operatorname{E}[X]+c\operatorname{E}[Y]                                             \\
    \mathbb{E}[X]\geq0\quad\text{if } & X\geq0\quad\Leftrightarrow f(x) = 0 \text{ for } x<0                                     \\
    \mathrm{Var}(X)                   & =\mathbb{E}[X^{2}]-{\mathbb{E}[X]}^{2}=\int_{-\infty}^{\infty}x^{2}f(x)dx-\mu_{X}^{2}    \\
    \mathrm{Var}(Y)                   & =\mathbb{E}[Y^{2}]-{\mathbb{E}[Y]}^{2}=\int_{-\infty}^{\infty}g^{2}(x)f(x)dx-\mu_{Y}^{2}
\end{align*}

\subsubsection{Quantiles}
\begin{itemize}
    \item $q_\alpha$ is an $\alpha$-quantile if $\mathbb{P}(X\leq q_\alpha)=\alpha,\text{ i.e. }F(q_\alpha)=\alpha$
    \item a $0.5$-quantile is called median\newline ($f$ symmetric $\leftrightarrow E[X]=$ median)
    \item the $q_{0.25}$ and $q_{0.75}$ are called lower and upper quartile
\end{itemize}
\ptitle{Discrete Random Variables}

For a discrete random variable $X$, $q_\alpha$ is an $\alpha$-quantile if
\begin{equation*}
    \mathbb{P}(X<q_\alpha)\le\alpha\le\mathbb{P}(X\le q_\alpha)
\end{equation*}
i.e. $q_\alpha$ is an interval where $F$ is equal to (value taken) or an $x$-value where $F$ jumps over (value not taken) the level $\alpha$.\\
% TBD: verify statements below
If the value of $\alpha$ is not taken this corresponds to
\begin{equation*}
    F(q_{\alpha-})< \alpha < F(q_{\alpha})
\end{equation*}
If the value of $\alpha$ is taken this corresponds to
\begin{equation*}
    F(q_{\alpha-})< \alpha = F(q_{\alpha})
\end{equation*}

\subsubsection{Transforming Random Variables}
\begin{itemize}
    \item Let $X$ be a random variable with cdf $F_X$ and density $f_X$
    \item $Y=g(X)$ for a differentiable strictly increasing transformation $g:\mathbb{R}\to\mathbb{R}$
\end{itemize}
Then
\begin{align*}
    f_Y(y)   & =(g^{-1})'(y)f_X(g^{-1}(y))=\frac{f_X(g^{-1}(y))}{g'(g^{-1}(y))}     \\
    F_{Y}(y) & =\mathbb{P}(g(X)\leq y)=\mathbb{P}(X\leq g^{-1}(y))=F_{X}(g^{-1}(y))
\end{align*}
i.e.\ for $f_Y(y)$ one can choose between 2 formulas depending on whether the derivative of $g$ or $g^{-1}$ is easier to calculate.

\ptitle{Linear Transformation}
\begin{align*}
    g(x)      & =a+bx,\;a\in\mathbb{R}\text,\;b>0           \\
    g^{-1}(y) & =\frac{y-a}b, g'(x)=b                       \\
    f_Y(y)    & =\frac1bf_X\left(\frac{y-a}b\right)         \\
    F_Y(y)    & =F_X(g^{-1}(y))=F_X\left(\frac{y-a}b\right)
\end{align*}

\ptitle{Exponential Transformation}
\begin{align*}
    X      & \sim\mathcal{N}(\mu,\sigma^2)                                                              \\
    Y      & =\exp(X),\;W_Y=(0,\infty) \text{(log-normal distribution)}                                 \\
    g(x)   & =\exp(x),\quad g^{-1}(y)=\log(y),\quad(g^{-1})'(y)=\frac{1}{y}                             \\
    f_Y(y) & =\frac{1}{\sqrt{2\pi\sigma^2}y}\exp\left(-\frac{{(\log(y)-\mu)}^2}{2\sigma^2}\right),\;y>0 \\
    F_Y(y) & =\begin{aligned}\Phi\left(\frac{\log(y)-\mu}\sigma\right),\;y>0\end{aligned}
\end{align*}

\subsubsection{Simulation of Random Variables}

\begin{itemize}
    \item Let $U\sim$ Unif$(0,1)$ and consider a cdf $F$
\end{itemize}
\begin{equation*}
    X=F^{-1}(U) \text{ has cdf }F
\end{equation*}
because
\begin{equation*}
    \mathbb{P}(X\leq x)=\mathbb{P}(F^{-1}(U)\leq x)=\mathbb{P}(U\leq F(x))=F(x)
\end{equation*}
as $U$ is a standard uniform distribution on $[0,1]$.

\ptitle{Simulating Discrete Random Variables}

\begin{itemize}
    \item If $F$ is discrete, $F^{-1}$ has to be understood in a generalized sense.
    \item Then $F^{-1}(U)$ still has cdf $F$
    \item As $F(x)$ only takes discrete values, the interval $[F(x_1), F(x_2))$ between two of these discrete values ($F(x_1), F(x_2)$) must be mapped to one and the same $x$. % ChkTex 9
    \item Conversely, one fixed value $F(x_1)$ must be mapped to an interval on the $x$-axis
\end{itemize}

\subsection{Continuous Distributions}

% just to debug
%\newcol{}

\subsubsection{Uniform Distribution}
Continuous version of the Laplace model of total ignorance

\renewcommand{\arraystretch}{1.3}
\setlength{\oldtabcolsep}{\tabcolsep}\setlength\tabcolsep{0pt}

\begin{tabularx}{\linewidth}{@{}p{0.5\linewidth}p{0.49\linewidth}@{}}
    \begin{minipage}{\linewidth}
        \noindent\begin{flalign*}{
             & X \sim \mathrm{Unif}(a,b),\; a,b\in \mathbb{R},\;a<b & \\
             & W=\left[a, b\right]=\left(a, b\right)                & \\
             & f(x)=\frac{1}{b-a},x\in[a,b]                         & \\
             & F(x)=\frac{x-a}{b-a},x\in[a,b]                       & \\
             & \mathbb{E}[X]=\frac{a+b}{2}                          & \\
             & \mathrm{Var}(X)=\frac{{(b-a)}^2}{12}                 &
            }\end{flalign*}
    \end{minipage}
     &
    \begin{minipage}{\linewidth}
        \input{plots/uniform_dist.tex}
    \end{minipage}
\end{tabularx}

\renewcommand{\arraystretch}{1}
\setlength\tabcolsep{\oldtabcolsep}

\subsubsection{Standard Normal Distribution}

\renewcommand{\arraystretch}{1.3}
\setlength{\oldtabcolsep}{\tabcolsep}\setlength\tabcolsep{0pt}

\begin{tabularx}{\linewidth}{@{}p{0.55\linewidth}p{0.45\linewidth}@{}}
    \begin{minipage}{\linewidth}
        \noindent\begin{flalign*}{
             & Z\sim\mathcal{N}(0,1),\;\mu = 0,\; \sigma^2 = 1           & \\
             & W=\mathbb{R}                                              & \\
             & \varphi(x)=f(x)=\frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}   & \\
             & \Phi(z)=\mathbb{P}(Z\leq z)= \int_{-\infty}^z\varphi(x)dx & \\
             & \mathbb{E}[Z] = 0                                         & \\
             & \mathrm{Var}(Z)=1                                         &
            }\end{flalign*}
    \end{minipage}
     &
    \begin{minipage}{\linewidth}
        \input{plots/std_normal_dist.tex}
    \end{minipage} \\
\end{tabularx}

\renewcommand{\arraystretch}{1}
\setlength\tabcolsep{\oldtabcolsep}

\subsubsection{Normal Distribution}
$Z$ is a random variable of the standard normal distribution.

\noindent\begin{flalign*}
     & X=\mu+\sigma Z\sim\mathcal{N}(\mu,\sigma^2),\;\mu\in \mathbb{R},\;\sigma^2  > 0                                            & \\
     & W=\mathbb{R}                                                                                                               & \\
     & f(x)= \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{{(x-\mu)}^2}{2\sigma^2}\right)                                         & \\
     & F(x)=\mathbb{P}(\mu+\sigma Z\leq x)=\mathbb{P}\left(Z\leq\frac{x-\mu}{\sigma}\right)=\Phi\left(\frac{x-\mu}{\sigma}\right) & \\
     & \mathbb{E}[X] = \mu                                                                                                        & \\
     & \mathrm{Var}(X)=\sigma^2                                                                                                   &
\end{flalign*}

\begin{center}
    \input{plots/normal_dist.tex}
\end{center}


\subsubsection{Exponential Distribution}

\renewcommand{\arraystretch}{1.3}
\setlength{\oldtabcolsep}{\tabcolsep}\setlength\tabcolsep{0pt}

\begin{tabularx}{\linewidth}{@{}p{0.45\linewidth}p{0.55\linewidth}@{}}
    \begin{minipage}[t]{\linewidth}
        \noindent\begin{flalign*}{
             & X \sim \mathrm{Exp}(\lambda), \;\lambda>0  & \\
             & W=[0,\infty)                               & \\ % ChkTex 9
             & f(x)=1_{\{x\geq0\}}\lambda e^{-\lambda x}  & \\
             & 1_{\{x\geq0\}} \text{: unit step function} & \\
             & F(x)=1-e^{-\lambda x},\;x\geq0             & \\
             & \mathbb{E}[X] = \frac{1}{\lambda}          & \\
             & \mathrm{Var}(X)=\frac{1}{\lambda^2}        &
            }\end{flalign*}
    \end{minipage}
     &
    \includegraphics[width=0.99\linewidth, align=t]{Cont_Exponential_Distribution.png}
    \\
\end{tabularx}

\renewcommand{\arraystretch}{1}
\setlength\tabcolsep{\oldtabcolsep}

\ptitle{Remarks:}
\begin{itemize}
    \item Simplest model for waiting times or life times of technical components
    \item The exponential distribution is \textbf{memoryless} on $\mathbb{R}^+$ i.e.\ waiting for $\Delta t$ again is equally likely after having already waited for $\Delta t$.
    \item If waiting times are $Exp(\lambda)$, then there are $Pois(\lambda t)$ events in intervals of length $t$
\end{itemize}


\subsubsection{Gamma Distribution}

\renewcommand{\arraystretch}{1.3}
\setlength{\oldtabcolsep}{\tabcolsep}\setlength\tabcolsep{0pt}

\begin{tabularx}{\linewidth}{@{}p{0.5\linewidth}p{0.49\linewidth}@{}}
    \begin{minipage}[t]{\linewidth}
        \noindent\begin{flalign*}{
             & X \sim \mathrm{Gamma}(\alpha, \beta), \;\alpha, \beta>0                                           & \\
             & W=[0,\infty)                                                                                      & \\ % ChkTex 9
             & f(x)=1_{\{x>0\}}\underbrace{\frac{\beta^\alpha}{\Gamma(\alpha)}}_{norm.} x^{\alpha-1}e^{-\beta x} & \\
             & \Gamma(\alpha)=\int_0^\infty x^{\alpha-1}e^{-x}dx                                                 & \\
             & F(x)=\int_0^x f(u)du,\;x\geq0                                                                     & \\
             & \mathbb{E}[X]=\frac{\alpha}{\beta}                                                                & \\
             & \mathrm{Var}(X)=\frac{\alpha}{\beta^2}                                                            &
            }\end{flalign*}
    \end{minipage}
     &
    \includegraphics[width=0.99\linewidth, align=t]{Cont_Gamma_Distribution.png} \\
    \\
\end{tabularx}

\renewcommand{\arraystretch}{1}
\setlength\tabcolsep{\oldtabcolsep}

\ptitle{Remarks:}
\begin{itemize}
    \item $\mathrm{Gamma}(1, \beta)=\mathrm{Exp}(\beta)$
    \item $\mathrm{Gamma}(\alpha, \beta)$ is a more general model for waiting times than Exp$( \lambda)$.
    \item It is also used to model severity of insurance claims.
\end{itemize}


\subsubsection{Beta Distribution}

\renewcommand{\arraystretch}{1.3}
\setlength{\oldtabcolsep}{\tabcolsep}\setlength\tabcolsep{0pt}

\begin{tabularx}{\linewidth}{@{}p{0.57\linewidth}p{0.43\linewidth}@{}}
    \begin{minipage}[t]{\linewidth}
        \noindent\begin{flalign*}{
             & X \sim \mathrm{Beta}(\alpha, \beta), \;\alpha, \beta>0                                     & \\
             & W=[0,1]                                                                                    & \\
             & f(x)=\frac{\Gamma(\alpha)\Gamma(\beta)}{\Gamma(\alpha+\beta)}x^{\alpha-1}{(1-x)}^{\beta-1} & \\
             & F(x)=\int_0^x f(u)du,\;x\geq0                                                              & \\
             & \mathbb{E}[X]=\int_0^1 xf(x)dx=\frac{\alpha}{\alpha + \beta}                               & \\
             & \mathrm{Var}(X)=\frac{\alpha\beta}{{(\alpha+\beta)}^2(\alpha+\beta+1)}
            }\end{flalign*}
    \end{minipage}
     &
    \includegraphics[width=0.99\linewidth, align=t]{Cont_Beta_Distribution.png}
    \\
\end{tabularx}

\renewcommand{\arraystretch}{1}
\setlength\tabcolsep{\oldtabcolsep}

\ptitle{Remarks:}
\begin{itemize}
    \item Caution: the given Beta distribution is given for the interval $[0,1]$. It can be rescaled to arbitrary intervals which then of course affects the formulas for $\mathbb{E}[X]$ and others.
    \item Flexible class of probability distributions on $[0,1]$ containing $\text{Unif}(0,1)$
\end{itemize}
