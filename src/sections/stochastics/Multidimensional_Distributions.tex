\section{Multidimensional Distributions}

In the following section, treats both disrete (\textsf{D}) distributions and continous (\textsf{C}) distributions with joint density.

\noindent\begin{flalign*}
    \mathsf{D}: &  &  & p(x,y)=\mathbb{P}(X=x,Y=y)                &  & \text{pmf.} & \\[.75em]
    \mathsf{C}: &  &  & f(x,y)\colon W_X\times W_Y\to\mathbb{R_+} &  & \text{pdf.} &
\end{flalign*}

\begin{center}
    \includegraphics[width=0.7\linewidth]{multi_dim_joint_dist.png}
\end{center}

\subsection{Properties}
\noindent\begin{flalign*}
    \mathsf{D}: &  & p(x,y)                 & \ge0\;\forall(x,y)                   & \\
                &  & p(x,y)                 & \le1\;\forall(x,y)                   & \\
                &  & \mathbb{P}((X,Y)\in A) & =\sum_{(x,y)\in A}p(x,y)             & \\[.75em]
    \mathsf{C}: &  & f(x,y)                 & \geq0\;\forall(x,y)                  & \\
                &  & f(x,y)                 & >1\text{ is possible}                & \\
                &  & \mathbb{P}((X,Y)\in A) & =\int\int_{(x,y)\in A}f(x,y)\;dx\,dy &
\end{flalign*}

\subsection{Marginal Distributions}

\ptitle{Remark}

Don't forget to renormalize the PMF/density.

\noindent\begin{flalign*}
    \mathsf{D}: &  & \mathbb P(X=x) & =\sum_{y\in W_Y}p(x,y) & \\
                &  & \mathbb P(Y=y) & =\sum_{x\in W_X}p(x,y) & \\[.75em]
    \mathsf{C}: &  & f_X(x)         & =\int_{W_Y}f(x,y)\;dy  & \\
                &  & f_Y(y)         & =\int_{W_X}f(x,y)\;dx  &
\end{flalign*}

\begin{center}
    \includegraphics[width=0.7\linewidth]{multi_dim_marginal_dist.png}
\end{center}

\subsection{Conditional Distributions}
\noindent\begin{flalign*}
    \mathsf{D}: &  & \mathbb P(Y=y|X=x) & = \frac{p(x,y)}{\mathbb P(X=x)},\quad \text{if }\mathbb P(X=x)>0 & \\[.75em]
    \mathsf{C}: &  & f(y|x)             & = \frac{f(x,y)}{f_X(x)}, \quad \text{if }f_X(x)>0                &
\end{flalign*}

\begin{center}
    \includegraphics[width=0.7\linewidth]{multi_dim_conditional_dist.png}
\end{center}

\subsection{Expectation}
\noindent\begin{flalign*}
    \mathsf{D}: &  & g\colon W_X\times W_Y & \to\mathbb{R},                            & \\
                &  & \mathbb{E}[g(X,Y)]    & =\sum_{x\in W_X,y\in W_Y}g(x,y)p(x,y)     & \\[.75em]
    \mathsf{C}: &  & g\colon W_X\times W_Y & \to\mathbb{R},                            & \\
                &  & \mathbb{E}[g(X,Y)]    & =\int_{W_X}\int_{W_Y}g(x,y)f(x,y)\;dx\,dy &
\end{flalign*}

\subsection{Conditional Expectation}
\noindent\begin{flalign*}
    \mathsf{D}: &  & \text{if }\mathbb{P}(X=x) & >0:                                       & \\
                &  & \mathbb{E}[Y\mid X=x]     & = \sum_{y\in W_Y}y\mathbb{P}(Y=y\mid X=x) & \\[.75em]
    \mathsf{C}: &  & \text{if }f_X(x)          & >0                                        & \\
                &  & \mathbb{E}[Y\mid X=x]     & =\int_{W_Y}y\,f(y\mid X=x)\;dy            &
\end{flalign*}

\ptitle{Remark}

$W_Y$ is restricted to the interval on which $X=x$ and vice versa.

\subsection{Independence}
\noindent\begin{flalign*}
    \mathsf{D}: &  & \mathbb{P}(X=x,Y=y)     & = \mathbb{P}(X=x)\mathbb{P}(Y=y) & \forall(x,y) & \\
                &  & \mathbb{P}(X=x\mid Y=y) & = \mathbb{P}(X=x)                & \forall(x,y) & \\
                &  & \mathbb{P}(Y=y\mid X=x) & =\newline \mathbb{P}(Y=y)        & \forall(x,y) & \\[.75em]
    \mathsf{C}: &  & f(x,y)                  & = f_X(x)f_Y(y)                   & \forall(x,y) & \\
                &  & f(x\mid y)              & =f_X(x)\newline                  & \forall(x,y) & \\
                &  & f(y\mid x)              & =f_Y(y)\newline                  & \forall(x,y) & \\
\end{flalign*}

\subsection{Variance, Covariance and Correlation}

\textbf{Covariance} and \textbf{correlation} measure the linear dependence between two random variables $X$ and $Y$.

\newpar{}

\ptitle{Covariance}
\begin{gather*}
    \mathrm{Cov}(X,Y) = \mathbb{E}[(X-\mu_X)(Y-\mu_Y)] = \mathbb{E}[XY]-\mu_X\mu_Y \\
    \\
    \mathrm{Cov}(X,X) = Var(X) = {\sigma_X}^2                                      \\
    \mathrm{Cov}(a+bX,c+dY) = bd\;\mathrm{Cov}(X,Y)
\end{gather*}

\newpar{}

\ptitle{Correlation}
\begin{gather*}
    \mathrm{Corr}(X,Y)        = \rho_{XY} = \frac{\mathrm{Cov}(X,Y)}{\sigma_X\sigma_Y} \qquad (\text{if } \sigma_X\sigma_Y > 0) \\
    \mathrm{Corr}(a+bX,c+dY)  = \mathrm{sign}(bd)\;\mathrm{Corr}(X,Y)
\end{gather*}

\newpar{}

\ptitle{Variance}
\begin{equation*}
    \mathrm{Var}(a_1 X_1+\cdot+a_n X_n) = \sum_{i=1}^{n}a_i^2\mathrm{Var}(X_i)+2\sum_{i<j}\mathrm{Cov}(X_i,X_j)
\end{equation*}

If $X$ and $Y$ are \textbf{independent}

\begin{gather*}
    \mathbb{E}[g(X)h(Y)] = \mathbb{E}[g(X)]\mathbb{E}[h(Y)] \\
    \mathrm{Cov}(X,Y) = 0 \\
    \mathrm{Var}(X+Y) = \mathrm{Var}(X) + \mathrm{Var}(Y) = \mathrm{Var}(X-Y)
\end{gather*}


\subsection{Two-Dimensional Normal Distribution}
The two-dimensional normal distribution is completely given by $\mu_X$, $\mu_Y$, $\sigma_X^2$, $\sigma_Y^2$ and the correlation $\rho_{XY}\in(-1,1)$ (or $Cov(X,Y)$).\\
The joint density is given by
\begin{footnotesize}
    \begin{equation*}
        f(x,y) = \frac{1}{2\pi\sqrt{\det(\bm{\Sigma})}}\exp\left(-\frac{1}{2}\begin{bmatrix}x-\mu_X & y-\mu_Y\end{bmatrix}\Sigma^{-1}\begin{bmatrix}x-\mu_X \\ y-\mu_Y \end{bmatrix}\right)
    \end{equation*}
\end{footnotesize}
with the \textbf{covariance matrix}
\begin{equation*}
    \Sigma = \begin{bmatrix}
        \mathrm{Var}(X)   & \mathrm{Cov}(X,Y) \\
        \mathrm{Cov}(X,Y) & \mathrm{Var}(Y)
    \end{bmatrix}
    =
    \begin{bmatrix}
        \sigma_X^2                & \rho_{XY}\sigma_X\sigma_Y \\
        \rho_{XY}\sigma_X\sigma_Y & \sigma_Y^2
    \end{bmatrix}
\end{equation*}

\begin{center}
    \includegraphics[width=\linewidth]{2d_normal_dist.png}
\end{center}

\newpar{}
\ptitle{Marginal Distributions}
\begin{align*}
    X & \sim\mathcal{N}(\mu_X, \sigma_X^2) \\
    Y & \sim\mathcal{N}(\mu_Y, \sigma_Y^2)
\end{align*}

\ptitle{Remarks}

\begin{itemize}
    \item For two-dimensional normal distributions one has always $Cov(X,Y)=0 \leftarrow X, Y$ are independent.
    \item One can therefore write $f(x,y)=f_X(x)f_Y(y)$ with the latter two being one-dimensional normal distributions.
\end{itemize}

\subsection{Mixtures Distributions}
Let $f_1,\dots, f_K$ be $d$-dimensional densities, then $f$ is again a density called mixture distribution.
\begin{gather*}
    f = \sum_{k=1}^{K} w_k f_k \\
    \text{with weights}\\
    w_1, \ldots, w_K \geq 0 \\
    \sum_{K=1}^{k}w_k = 1
\end{gather*}
