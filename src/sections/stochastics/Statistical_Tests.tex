\section{Statistical Tests}
% Statistical tests answer the second main question of statistical inference.
Statistical tests can be used to check if an \textit{assumption} or \textit{parameter} is consistent with the observed data i.e.\ the \textit{decision rule is objectively reproducible}.

\ptitle{Remarks}

\begin{itemize}
    \item One must fix $H_0$, $H_A$ before data collection
    \item Hypotheses can be formed based on data, but then new data needs to be collected to test it
    \item The alternative $H_A$ can either be one- or two-sided
    \item \textbf{One-sided} tests reject $H_0$ sooner on one side but is blind on the other (greater power)
    \item \textbf{Two-sided} tests reject detect effect on both sides
\end{itemize}

\ptitle{Error Types}\label{error_types}

\renewcommand{\arraystretch}{1.3}
\setlength{\oldtabcolsep}{\tabcolsep}\setlength\tabcolsep{3pt}

\begin{tabularx}{\linewidth}{@{}c c >{\centering\arraybackslash}p{.3\linewidth} >{\centering\arraybackslash} p{.3\linewidth}@{}}
                                                               &       & \multicolumn{2}{c}{Decision}                                          \\
                                                               &       & $H_0$                             & $H_A$                             \\
    \cmidrule{3-4}
    \multirow{2}{*}{\begin{sideways}Truth\;\;\;\end{sideways}} & $H_0$ & \checkmark{}                      & Type 1 error\par (false positive) \\
                                                               & $H_A$ & Type 2 error\par (false negative) & \checkmark{}
\end{tabularx}

% \begin{center}
%     \includegraphics[width=0.6\linewidth]{error_types.png}
% \end{center}

\renewcommand{\arraystretch}{1}
\setlength\tabcolsep{\oldtabcolsep}

\subsection{Binomial Test}
$X\sim \mathrm{Bin}(n,p)$

\ptitle{Null Hypothesis}:
\noindent\begin{equation*}
    H_0: p=0.5\quad \textit{status quo, no effect}
\end{equation*}
\ptitle{Alternative Hypothesis}

\noindent\begin{equation*}
    H_A: p>0.5
\end{equation*}
% Remark: Depending on the experiment one could also test $p<0.5$ or have another value than $0.5$. % I think this is clear

\ptitle{Rejection Region}

The \textit{rejection region} can be chosen by setting an \textit{a priori} fixed \textbf{significance level} (probability level) $\alpha$ such that
\noindent\begin{equation*}
    \mathbb{P}(\text{type 1 error}) = \mathbb{P}_{H_0}(X\geq c) \leq \alpha
\end{equation*}
The \textit{threshold} $c\in \mathbb{R}$ is then used to reject $H_0$ if $X\geq c$.

Typical values of $\alpha$ are 0.05, 0.01, 0.001 (usually smaller the larger $n$ is).

\newpar{}
\ptitle{Remarks on the Rejection Region}
\begin{itemize}
    \item For large $n$ use a \textit{normal distribution}
    \item Small $c$ increase the probability of type 1 errors, large $c$ for type 2 errors respectively.
    \item $\mathbb{P}_{H_0}(X\geq c)$ is the probability that we reject $H_0$ (because we observe $X\geq c$) even though it is true.
    \item The form of the rejection region depends on $H_A$:
          \begin{itemize}
              \item $H_A: p>p_0 \Rightarrow \text{smallest }c\text{ s.t. } \mathbb{P}(X\geq c)\leq \alpha$
              \item $H_A: p<p_0 \Rightarrow \text{largest }c\text{ s.t. } \mathbb{P}(X\leq c)\leq \alpha$
              \item $H_A: p\neq p_0 \Rightarrow \text{largest } c_1, \text{smallest }c_2 \text{ s.t.}$\newline
                    $ \mathbb{P}(X\leq c_1)\leq \frac{\alpha}{2} \text{ and }\mathbb{P}(X\geq c_2)\leq \frac{\alpha}{2}$
          \end{itemize}
    \item One controls the probability of type 1 errors (not type 2).
\end{itemize}

\begin{examplesection}[Coin flip example]
    10 coin flips of an assumably unfair coin.
    \noindent\begin{gather*}
        X\sim \mathrm{Bin}(10,p),\quad \alpha = 0.05\quad H_0:p=0.5,\quad H_A: p>0.5\\
        \to c=9,\quad \text{rejection region: }\{9,10\}
    \end{gather*}
    $x=8$ is observed: there is not enough evidence to reject $H_0$.
\end{examplesection}

\subsection[z-Test]{z-Test ($X\sim\mathcal{N}$)}
$X\sim\mathcal{N}(\mu,\sigma^2), \quad \sigma$ is known, $\mu$ unknown.

\ptitle{Null Hypotesis}
\noindent\begin{equation*}
    H_0:\mu=\mu_0
\end{equation*}
thus under $H_0$
\noindent\begin{equation*}
    \bar{X}_n\sim\mathcal{N}\left(\mu_0,\frac{\sigma^2}n\right), \quad Z=\frac{\bar{X}_n-\mu_0}{\sigma/\sqrt{n}}\sim\mathcal{N}(0,1)
\end{equation*}

\ptitle{Alternative Hypothesis and Rejection Region}

\renewcommand{\arraystretch}{1.3}
\setlength{\oldtabcolsep}{\tabcolsep}\setlength\tabcolsep{6pt}

\begin{tabularx}{\linewidth}{@{}p{0.15\linewidth}lll@{}}
    $H_A$            & $\mu\neq\mu_0$       & $\mu>\mu_0$      & $\mu<\mu_0$               \\
    Rejection region & $|Z|>z_{1-\alpha/2}$ & $Z>z_{1-\alpha}$ & $Z<z_\alpha=-z_{-\alpha}$

    
    
    
    
    
    
    
    
    
    
\end{tabularx}

\renewcommand{\arraystretch}{1}
\setlength\tabcolsep{\oldtabcolsep}

If observation lies within $Z$, reject $H_0$ in favor of $H_A$.

\ptitle{Remarks}
\begin{itemize}
    \item check~\ref{norm_approx_bin} and~\ref{norm_approx_poiss} if $X\sim \mathrm{Bin(n,p)}$ or $X\sim \mathrm{Pois}(\lambda)$
          % \item We assume that $\bar{X}_n$ is normally distributed (CLT)
          % \item Hence, the \textit{test statistic} $Z$ is standard normally distributed (transform of $\mathcal{N}$) and we can use tabular values of the standard normal to perform the test
          % \item The test checks the probability of having a transformed arithmetic mean $|Z|>z_{1-\alpha/2}$ under $H_0:\mu=\mu_0$
\end{itemize}

\subsection[t-Test]{t-Test ($X\sim\mathcal{N}$)}
$X\sim\mathcal{N}(\mu,\sigma^2), \quad \sigma, \mu$ are unknown.

\ptitle{Null Hypotesis}
\noindent\begin{equation*}
    H_0:\mu=\mu_0
\end{equation*}

\ptitle{Test Statistic}
\noindent\begin{align*}
    T        & =\frac{\bar{X}_n-\mu_0}{s_n/\sqrt{n}},                      &  & s_n^2=\frac{1}{n-1}\sum_{i=1}^{n}{(X_i-\bar{X_n})}^2                       \\
    t_\nu(x) & = c_\nu{\left(1+\frac{x^2}{\nu}\right)}^{-\frac{\nu+1}{2}}, &  & c_\nu=\frac{\Gamma(\frac{\nu+1}{2})}{\sqrt{\pi\nu}\,\Gamma(\frac{\nu}{2})}
\end{align*}

\setlength{\oldtabcolsep}{\tabcolsep}\setlength\tabcolsep{0pt}

\begin{tabularx}{\linewidth}{@{}p{0.45\linewidth}X@{}}
    \noindent\includegraphics[width=0.99\linewidth, align=t]{t-distribution.png}
     & 
    \begin{equation*}
        t_\infty = \mathcal{N}(0,1)
    \end{equation*}
\end{tabularx}

\setlength\tabcolsep{\oldtabcolsep}

\textbf{Remarks}:
\begin{itemize}
    \item T is \textit{t-distributed} with $n-1$ degrees of freedom under $H_0$
          % \item Use tables to determine quantiles
          % \item As it is not an exponential, a $t$-distribution (with low $\nu$) has a larger probability of very small/large observations (larger tails) than a normal distribution\
\end{itemize}

\newpar{}
\ptitle{Alternative Hypothesis and Rejection Region}

\renewcommand{\arraystretch}{1.3}
\setlength{\oldtabcolsep}{\tabcolsep}\setlength\tabcolsep{6pt}

\begin{tabularx}{\linewidth}{@{}p{0.15\linewidth}llX@{}}
    $H_A$            & $\mu\neq\mu_0$           & $\mu>\mu_0$          & $\mu<\mu_0$                                       \\
    Rejection region & $|T|>t_{n-1,1-\alpha/2}$ & $T>t_{n-1,1-\alpha}$ & $T<t_{n-1,\alpha}\newline \quad=-t_{n-1,-\alpha}$

    
    
    
    
    
    
    
    
    
    
\end{tabularx}

\renewcommand{\arraystretch}{1}
\setlength\tabcolsep{\oldtabcolsep}

If observation lies within $T$, reject $H_0$ in favor of $H_A$.

\subsection[p-Value]{$p$-Value}
The $p$-value is an alternative formulation to the rejection region with equal result. If the $p$-value is $\leq \alpha$, the test statistic is in the rejection region.
\begin{equation*}
    \text{reject } H_0 \text{ if } p\text{-value} \leq \alpha
\end{equation*}

The $p$-value is a probability that a test statistic value of at least as extreme as the observed one ($T$) occurs under the assumption that $H_0$ holds.

\textbf{One-sided $t$-test}
\begin{equation*}
    p\text{-value} = 1-F(T) = \int_{T}^{\infty}\text{PDF }dx \qquad \text{testing above}
\end{equation*}
or
\begin{equation*}
    p\text{-value} = F(T) = \int_{-\infty}^{T}\text{PDF }dx \qquad \text{testing below}
\end{equation*}
\textbf{Two-sided $t$-test}
\begin{equation*}
    p\text{-value} = 2(1-F(T)) =  2\int_{|T|}^{\infty}\text{PDF }dx
\end{equation*}

\newpar{}
\ptitle{Testing via $p$-value}
\begin{enumerate}
    \item fix the significance level $\alpha$
    \item compute the $p$-value under $H_0$ from the observed data
    \item reject $H_0$ if $p$-value $\leq \alpha$
\end{enumerate}

\newpar{}
\ptitle{Dangers}

\begin{itemize}
    \item Multiple testing/$p$-value hacking: if $H_0$ holds, one expects a critical p-value (i.e.\ a $p$-value) in $\alpha \times 100\%$ of the tests. So, if one conducts enough tests, one always finds a significant $p$-value. The guarantee $\mathbb{P}(\text{type 1 error}) \leq \alpha$ is only valid for a single test. For multiple tests one has to apply a testing correction like the \textbf{Bonferroni correction} shown in Section~\ref{bonferroni_correction}.
    \item Relevance of small $p$-values: the $p$-value does not say anything about the size of an effect (compute confidence interval).
\end{itemize}
% \newpar{}
% \ptitle{Differentiation From Aforementioned Methods}

% \begin{itemize}
%     \item Using the significance level, we computed $F^{-1}(\alpha)$ (e.g.\ $z_{1-\alpha}$) and checked whether our test statistic lied in the rejection region given by this value (i.e.\ one compares values for $X$).
%     \item Using the $p$-value we calculate the probability $p=\mathbb{P}_{H_0}$ of obtaining a value as least as extreme as our test statistic (i.e.\ one compares probabilities).
%     \item The methods are equivalent but formulated differently.
% \end{itemize}

\subsection{Bonferroni Correction for Multiple Testing}\label{bonferroni_correction}
If one performs $N$ tests, the significance level $\alpha$ has to be replaced by $\alpha/N$.
\begin{equation*}
    \mathbb{P}(\text{type 1 error in at least one of the }N\text{ tests}) \leq \sum_{i=1}^{N}\frac{\alpha}{N}=\alpha
\end{equation*}

\subsection{Power of a Test}
The \textbf{power} of a test is the probability that the test correctly rejects the $H_0$ hypothesis.
\begin{gather*}
    \mathbb{P}(\text{test rejects }H_0) = 1-\mathbb{P}(\text{type 2 error})
\end{gather*}
given $H_A$ is true. %$\mathbb{P}(\text{type 2 error})$ is visible in paragraph~\ref{error_types}.

\subsection{Sign Test}
The sign test is a test for \textbf{non-normally} distributed data.
\newpar{}
$X \sim F_\mu$ with \textbf{median} $\mu$.
\newpar{}
\ptitle{Null Hypotesis}
\begin{equation*}
    H_0:\mu = \mu_0
\end{equation*}
\newpar{}
\ptitle{Idea}

Count the number of $Q$ of $X_i$ that are larger than $\mu_0$ or count the number of positive signs of $X_i-\mu_0$. Then apply a binomial test \textit{to the signs} of the data.
\begin{equation*}
    Q \sim \text{Bin}(n,p) \quad p=\mathbb{P}(X_i > \mu_0)
\end{equation*}
Under $H_0$
\begin{equation*}
    \mathbb{P}(X > \mu_0)=0.5 \quad \text{definition of the median}
\end{equation*}
Therefore
\begin{equation*}
    H_0: p=p_0=0.5
\end{equation*}

\newpar{}
\ptitle{Remarks}
\begin{itemize}
    \item Advantage: weak assumptions
    \item Drawback: the power is low compared to the $Z$-test or the $t$-test
\end{itemize}

\begin{examplesection}[Sign Test Example]
    \noindent\begin{gather*}
        x_i = \{1003.05, 1004.11, 1002.61, 1002.51, 1003.99, \ldots \\
        \qquad 1003.32, 999.7, 1002.95, 1001.9, 1002.47\}
    \end{gather*}
    Null hypothesis and significance level
    \begin{equation*}
        H_0: \mu=\mu_0=1000, \qquad \alpha=0.05
    \end{equation*}
    Possible values of the test statistic ($\#$ values above mean)
    \begin{equation*}
        Q: \{0,1,\ldots,10\}
    \end{equation*}
    \newpar{}
    Observed value of
    \begin{equation*}
        Q: \quad q=9
    \end{equation*}
    \newpar{}
    Values which (two sided) are at least as extreme as $9$:
    \begin{equation*}
        \{0,1,9,10\}
    \end{equation*}
    \newpar{}
    $p$-value
    \begin{align*}
         & \mathbb{P}_{p=0.5}(Q\in\{0,1,9,10\})=2\times\mathbb{P}_{p=0.5}(Q\in\{0,1\}) \\
         & = 2\times\left(0.5^{10} + {10 \choose 1} 0.5^1 0.5^9\right)                 \\
         & = 2\times\left(0.5^{10}+ 10\times 0.5^{10}\right)= 0.0215
    \end{align*}
    $p$-value $\leq 0.05 \rightarrow$ significant result
    
    $H_0$ median $= 1000$ is \textbf{rejected}.
\end{examplesection}

\subsection{Comparison of Two Samples}

% If one compares samples there are two main categories:

\subsubsection{Paired Samples}

\begin{itemize}
    \item Two observations for each experimental unit.
    \item The two observations are not independent.
    \item Random fluctuations between experimental units cancel out. (This is called blocking and increases the power of the test.)
    \item If a paired test sees that that B is higher than A for all experimental units. It is more likely to find a significant difference. An unpaired test would not find a significant difference.
\end{itemize}

\begin{center}
    \includegraphics[width=0.6\linewidth]{paired_samples.png}
\end{center}

\newpar{}
\paragraph{Testing with Paired Samples}

For each experiment the difference between the two test results is used.
\begin{itemize}
    \item This creates a new simple dataset instead of paired data: the differences are a new random variable.
    \item One can then apply a standard test to the simple data (e.g.\ $Z$- or $t$-test for normally distributed data, sign test for non-normally distributed data).
\end{itemize}

\newcol{}
\subsubsection{Unpaired Samples}

\begin{itemize}
    \item Two groups of different experimental units.
    \item Each experimental unit yields an observation.
    \item The observations are independent.
\end{itemize}

\begin{center}
    \includegraphics[width=0.6\linewidth]{unpaired_samples.png}
\end{center}

\paragraph{t-Testing with Unpaired Samples}
\ptitle{Test Random Variable}

% TBD: is the 1st statement correct?
For unpaired samples one can't perform a test on the individual signs (in general, $m\neq n$) of $X_i-Y_i$ but instead performs a test on the arithmetic means. One assumes that
\begin{gather*}
    X_1,\dots,X_n\text{ i.i.d. }\mathcal{N}(\mu_X,\sigma^2) \qquad Y_1,\dots,Y_m\text{ i.i.d. }\mathcal{N}(\mu_Y,\sigma^2) \\
    \mu_X,\mu_Y,\sigma^2 \text{ unknown but $X_i$ and $Y_i$ have same $\sigma^2$}
\end{gather*}
Then, one creates a new \textit{normally distributed} random variable
\begin{equation*}
    W = \bar{X}_n-\bar{Y}_m=\frac{1}{n}\sum_{i=1}^{n} X_i-\frac{1}{m}\sum_{j=1}^{m} Y_j
\end{equation*}
(average score difference) with
\begin{align*}
    \mathbb{E}[W]   & =\mathbb{E}[\bar{X}_n]-\mathbb{E}[\bar{Y}_m]=\mu_X-\mu_Y = \mu_W                              \\
    \mathrm{Var}(W) & =\mathrm{Var}(\bar{X}_n)+\mathrm{Var}(\bar{Y}_m)=\sigma^2\left(\frac{1}{n}+\frac{1}{m}\right)
\end{align*}

\ptitle{Testing}

We get the test statistic (prototype)
\begin{equation*}
    Z=\frac{W-\mu_W}{\sigma_W}=\frac{\bar{X}_n-\bar{Y}_m-(\mu_X-\mu_Y)}{\sqrt{\sigma^2\left(\frac{1}{n}+\frac{1}{m}\right)}}\sim\mathcal{N}(0,1)
\end{equation*}

Under the null hypothesis (assumes that e.g.\ a new and an old device perform equally)
\begin{equation*}
    Z=H_0{:}\quad \mu_W = \mu_X-\mu_Y=0\quad(\mu_X=\mu_Y)
\end{equation*}
implying
\begin{equation*}
    \frac{W}{\sigma_W} = \frac{\bar{X}_n-\bar{Y}_m}{\sigma\sqrt{\frac{1}{n}+\frac{1}{m}}}\sim\mathcal{N}(0,1)
\end{equation*}
We still need an estimator for $\sigma^2$
\begin{align*}
    S_{\text{pool}}^2 & =\frac{1}{n+m-2}\left(\sum_{i=1}^n{(X_i-\bar{X}_n)}^2+\sum_{j=1}^m{(Y_j-\bar{Y}_m)}^2\right) \\
                      & =\frac{1}{n+m-2}\Bigg((n-1)S_X^2+(m-1)S_Y^2\Bigg)
\end{align*}
Therefore, the test statistic (under $H_0$) can be reformulated as
\begin{equation*}
    T=\frac{\bar{X}_n-\bar{Y}_m}{S_\text{pool}\sqrt{\frac1n+\frac1m}}\sim t_{n+m-2}
\end{equation*}

\ptitle{Remarks}
\begin{itemize}
    \item $S_{\text{pool}}^2$ weights the individual squared deviations from the means depending on the sample sizes of $m,n$.
    \item One avoids complications due to $m \neq n$ by always either relating $\bar{X}_n$ to $\bar{Y}_m$ or $X_i$ to $\bar{X}_n$ (same for $Y_i$).
    \item The prototype test statistic still assumes a known $\sigma^2$ and hence a normal distribution. The change to $t$ happens using $S_\text{pool}$.
    \item The test statistic has $n+m-2$ degrees of freedom.
    \item $\mathrm{Var}(W)$ contains no covariance term as independence of $\bar{X}_n,\bar{Y}_n$ is assumed.
\end{itemize}

\paragraph{z-Testing with Unpaired Samples}

Use the prototype test statistic
\begin{equation*}
    Z = \frac{W-\mu_W}{\sigma_W}=\frac{\bar{X}_n-\bar{Y}_m-(\mu_X-\mu_Y)}{\sqrt{\sigma^2\left(\frac{1}{n}+\frac{1}{m}\right)}}\sim\mathcal{N}(0,1)
\end{equation*}
which can be used directly as $\sigma^2$ is known (in contrast to t-testing).
Then use $z$ quantiles instead of $t$ quantiles.