\section{Point Estimates}
Point estimates answer the first main question of statistical inference.
A \textit{point estimation} can be either
\begin{itemize}
    \item A function $\hat{\theta}: \mathbb{R}^n\to \mathbb{R}^r$
    \item A best guess $\hat{\theta} = \hat{\theta}(x_1,\ldots, x_n)$
    \item A random variable/ vector $\hat{\theta} = \hat{\theta}(X_1,\ldots, X_n)$
\end{itemize}

\subsection{Method of Moments (MoM)}
MoM assumes that a certain estimator can be expressed as a function of moments.
The \textbf{moments} of a random variable $X$ are
\noindent\begin{equation*}
    \mu_k=\mathbb{E}[X^k],k=1,2,\ldots
\end{equation*}
The $\mu_k$ can be estimated with \textit{empirical moments} $m_k$
\noindent\begin{equation*}
    m_k=\frac{1}{n}\sum_{i=1}^n {x_i}^k
\end{equation*}
Hence, one replaces the $\mu_k$ by $m_k$ to calculate the estimator.

\begin{examplesection}[Examples of MoM]
    $X_i \sim \text{Pois}(\lambda)$ and i.i.d.
    \noindent\begin{align*}
        \lambda       & = \mathbb{E}[X_i] = \mu_i            \\
        \hat{\lambda} & = m_1 =\frac{1}{n}\sum_{i=1}^n {x_i}
    \end{align*}
    or
    \noindent\begin{align*}
        \lambda       & = \mathrm{Var}(X_i)=\mu_2-\mu_1^2                                                     \\
        \hat{\lambda} & =m_2-m_1^2=\frac{1}{n}\sum_{i=1}^n x_i^2-{\left(\frac{1}{n}\sum_{i=1}^n x_i\right)}^2
    \end{align*}

    $X_i \sim \mathcal{N}(\mu,\sigma^2)$
    \noindent\begin{align*}
        \mu            & = \mu_1, \quad \sigma^2 = \mu_2-{\mu_1}^2                                             \\
        \hat{\mu}      & =m_1=\frac{1}{n}\sum_{i=1}^n x_i                                                      \\
        \hat{\sigma}^2 & =m_2-m_1^2=\frac{1}{n}\sum_{i=1}^n x_i^2-{\left(\frac{1}{n}\sum_{i=1}^n x_i\right)}^2
    \end{align*}
\end{examplesection}

\subsection{Maximum Likelihood Estimation (MLE)}
MLE assumes i.i.d. $X_i$.
\begin{enumerate}
    \item choose distribution
    \item Define \textit{Likelihood} as a function of parameter $\lambda$
          \noindent\begin{equation*}
              L(\lambda) = \mathbb{P}(x_1,\ldots ,x_n|\lambda) \stackrel{i.i.d.}{=} \prod_{i=1}^n\mathbb{P}(X_i=x_i\mid\lambda)
          \end{equation*}
    \item Find the \textit{maximum likelihood estimator}
          \noindent\begin{equation*}
              \hat{\lambda} = \mathrm{argmax}_\lambda L(\lambda)
          \end{equation*}
          i.e.\ find the $\lambda$ for which measuring the given data is most likely.
    \item (optional) define \textit{log-likelihood function}
          \noindent\begin{align*}
              l(\lambda)    & = \ln(L(\lambda))                    \\
              \hat{\lambda} & = \mathrm{argmax}_\lambda l(\lambda)
          \end{align*}
\end{enumerate}

\begin{examplesection}[Examples of MLE]
    $X_i \sim \text{Pois}(\lambda)$
    \noindent\begin{align*}
        L(\lambda)          & =\prod_{i=1}^n e^{-\lambda}\frac{\lambda^{x_i}}{x_i!}                                                                               \\
        l(\lambda)          & =\sum_{i=1}^n\log\left(e^{-\lambda}\frac{\lambda^{x_i}}{x_i!}\right)=\sum_{i=1}^n\left\{x_i\log(\lambda)-\log(x_i!)-\lambda\right\} \\
        l^{\prime}(\lambda) & = 0 \Leftrightarrow \sum_{i=1}^n\frac{x_i}\lambda-n = 0 \Rightarrow \hat{\lambda} =\frac1n\sum_{i=1}^n x_i
    \end{align*}
\end{examplesection}

\subsubsection{Continuous Distribution with a Density}
For continuous distributions with a density, the MLE can be found using $f$ instead of $p$ with
\noindent\begin{equation*}
    \hat{\theta}=\operatorname{argmax}_\theta\sum_{i=1}^n\log f_\theta(x_i)
\end{equation*}
If $\theta$ is $r$-dimensional, all partial derivatives can be set to 0:
\noindent\begin{equation*}
    \frac{\partial l(\theta)}{\partial\theta_1}=\cdots=\frac{\partial l(\theta)}{\partial\theta_r}=0\quad\leadsto\text{this gives }r\text{ equations}
\end{equation*}

\begin{examplesection}[Examples]
    $X_n\sim\mathcal{N}(\mu,\sigma^2=v)$
    \noindent\begin{align*}
        -l(\mu,v)                                & =\frac12\sum_{i=1}^n\left\{\frac{{(x_i-\mu)}^2}v+\log(2\pi)+\log(v)\right\}         \\
        0=-\frac{\partial l(\mu,v)}{\partial\mu} & =\sum_{i=1}^n\frac{\mu-x_i}v\quad\Rightarrow\quad \hat{\mu}=\frac1n\sum_{i=1}^n x_i \\
        0=-\frac{\partial l(\mu,v)}{\partial v}  & =\frac12\sum_{i=1}^n\left\{-\frac{{(x_i-\mu)}^2}{v^2}+\frac1v\right\}               \\
                                                 & \Rightarrow\quad\hat{v}=\frac1n\sum_{i=1}^n{(x_i-\hat{\mu})}^2
    \end{align*}
    minimizes the \textbf{negative log-likelyhood} function.\\
    $X_n\sim \mathrm{Unif}(0,\theta)$ is an example of a \textbf{non-differentiable} likelihood function.
    \noindent\begin{align*}
        f_\theta(x)        & =\begin{cases}
                                  1/\theta & \quad\mathrm{if~}0\leq x\leq\theta \\
                                  0        & \quad\mathrm{else}
                              \end{cases}                                                  \\
        L(\theta)          & =\prod_{i=1}^n f_\theta(x_i)=\begin{cases}
                                                              \prod_{i=1}^n1/\theta=1/\theta^n & \mathrm{if~}0\leq x_i\leq\theta \\
                                                              0                                & \mathrm{else}
                                                          \end{cases} \\
        \theta(x_1,\ldots) & =\max_i x_i \text{ all } x_i \text{ have to lie within } [0,\theta]
    \end{align*}
\end{examplesection}

\subsection{General Estimators}
For i.i.d.\ random variables with \textit{unknown distributions} the \textit{general estimators}
\noindent\begin{align*}
    \hat{\mu}_{X}    & =\bar{X}_{n}=\frac1n\sum_{i=1}^{n}X_{i}             \\
    \hat{\sigma}_X^2 & =s_n^2=\frac1{n-1}\sum_{i=1}^n{(X_i-\hat{\mu}_X)}^2
\end{align*}
can be used. These estimators have the following properties:
\noindent\begin{align*}
    \mathbb{E}[\hat{\mu}_x]                                       & =\mathbb{E}[\bar{X}_n]=\frac1n\sum_{i=1}^n\mathbb{E}[X_i]=\mu_X                                    \\
    \mathrm{Var}(\hat{\mu}_X)                                     & =\mathrm{Var}(\bar{X}_n)=\frac{n\mathrm{Var}(X_1)}{n^2}=\frac{\sigma_X^2}n                         \\
    \underbrace{\sigma_{\hat{\mu}_{X}}}_{\textsf{of mean estim.}} & =\sqrt{\mathrm{Var}(\hat{\mu}_{X})}=\underbrace{\frac{\sigma_{X}}{\sqrt{n}}}_{\textsf{of dataset}} \\
    \mathbb{E}[\hat{\sigma}_X^2]                                  & =\frac1{n-1}\mathbb{E}\left[\sum_{i=1}^n{\left(X_i-\bar{X}_n\right)}^2\right] = \sigma^2_X
\end{align*}

\textbf{Remark} $\hat{\mu}_X, \hat{\sigma}_X^2$ are \textbf{unbiased} i.e.\ $Bias(\hat{\theta})=\mathbb{E}[\hat{\theta}-\theta]=0$
