\section{Sample Spaces and Probability Measures}
\subsection{Notation}
\noindent\begin{align*}
     & \omega                 &  & \text{Possible outcome}                               \\
     & \Omega                 &  & \text{Sample space}                                   \\
     & A \subseteq \Omega     &  & \text{Event}                                          \\
     & {\{x_1 \dots x_n \}}^k &  & \text{all sequences of length k using elements } x_i. \\
     & \mathbb{P}(A)          &  & \text{Probability that A occurs.}
\end{align*}
\subsection{De Morgan's Laws}
\noindent\begin{align*}
    {(A\cup B)}^C = A^C\cap B^C \\
    {(A\cap B)}^C = A^C\cup B^C
\end{align*}

\subsection{Axioms of Probability Theory}
\begin{enumerate}
    \item $0\leq \mathbb{P}(A)\leq 1$
    \item $\mathbb{P}(\Omega)$ = 1
    \item $\mathbb{P}\left(\cup_{i\geq 1} A_i\right) = \mathbb{P}\underbrace{(A_1 \cup A_2 \cup \dots)}_{\text{countably infinite}} = \sum_{i\geq 1} \mathbb{P}(A_i)$\\
          if $A_{i} \cap A_{j} = \emptyset \; \forall i \ne j$ (piecewise disjoint)
\end{enumerate}
A sample space $\Omega$ with a probability measure $\mathbb{P}$ forms a \textbf{probability space}.

\subsubsection{Further rules from the Axioms}\label{sssec:rules_from_axioms}
\noindent\begin{align*}
     & \mathbb{P}(\emptyset) = 0                                                                                                                                  \\
     & \mathbb{P}  \underbrace{(A_1 \cup \dots \cup A_n)}_{\text{finite}} = \sum_{i=1}^{n} \mathbb{P}(A_i) & \text{if } A_i\cap A_j = \emptyset\, \forall i\neq j \\
     & \mathbb{P}(A^C) = 1-\mathbb{P}(A)                                                                                                                          \\
     & \mathbb{P}(A\cup B) = \mathbb{P}(A)+\mathbb{P}(B) - \mathbb{P}(A\cap B)                                                                                    \\
     & \mathbb{P}(A_1 \cup \dots \cup A_n) \leq \mathbb{P}(A_1)+\dots \mathbb{P}(A_n)                                                                             \\
     & \mathbb{P}(B) \leq \mathbb{P}(A)                                                                    & \text{if } B\subseteq A                              \\
     & \mathbb{P}(A\backslash B) = \mathbb{P}(A)-\mathbb{P}(B)                                             & \text{if } B\subseteq A
\end{align*}

\subsection{Discrete Probability Spaces}
A discrete probability space has at most countably many different elements. As the outcomes exclude each other:
\noindent\begin{align*}
    \mathbb{P}(A) & = \mathbb{P}\left(\bigcup_{\substack{\omega_i \in A}}\{\omega_i\}\right)= \sum_{\substack{\omega_i \in A}} \mathbb{P}(\omega_i)
\end{align*}
\subsubsection{Laplace Model}
In the Laplace model, all possible outcomes have the same probability.
\noindent\begin{align*}
    \mathbb{P}(\omega_i) & = \frac{1}{|\Omega|}                                             \\
    \mathbb{P}(A)        & = \sum_{\omega_i \in A}\frac{1}{|\Omega|} = \frac{|A|}{|\Omega|}
\end{align*}

\section{Independence, Conditional Prob.\ and Bayes}
\subsection{Independence}
\ptitle{General}

\noindent\begin{align*}
    A \cap B = \emptyset                          & \Rightarrow \mathbb{P}(A\cap B) = 0                                 \\
    B \subseteq A                                 & \Rightarrow \mathbb{P}(A\cap B) = \mathbb{P}(B)                     \\
    0 \leq \mathbb{P}(A\cap B)                    & \leq \min\{\mathbb{P}(A), \mathbb{P}(B)\}                           \\
    \max\{0, 1-\mathbb{P}(A^C) -\mathbb{P}(B^C)\} & \leq \mathbb{P}(A\cap B) \leq \min\{ \mathbb{P}(A), \mathbb{P}(B)\}
\end{align*}

\ptitle{Independent}

$A$ and $B$ are independent if
\noindent\begin{equation*}
    \mathbb{P}(A\cap B) = \mathbb{P}(A)\mathbb{P}(B)
\end{equation*}
For several events $A_{1\dots n}$ and each choice $A_{i1\dots ik}$ with $k\leq n$
\noindent\begin{equation*}
    \mathbb{P}(A_{ij} \cap \dots \cap A_{ik}) = \mathbb{P}(A_{ij}) \cdots \mathbb{P}(A_{ik})
\end{equation*}
holds. This implies that each subset of events must be independent as well.

\subsection{Total Probability}
\noindent\begin{align*}
    \mathbb{P}(A)     & =\mathbb{P}(A\cap B)+\mathbb{P}(A\cap B^c)                                           \\
                      & =\mathbb{P}(A\mid B)\mathbb{P}(B)+\mathbb{P}(A\mid B^c)\mathbb{P}(B^c)               \\\\
    \mathbb{P}(A)     & =\sum_{i=1}^k\mathbb{P}(A\cap B_i) =\sum_{i=1}^k\mathbb{P}(A\mid B_i)\mathbb{P}(B_i) \\
    \text{if } \Omega & =B_1\cup\cdots\cup B_k,\quad\text{with }B_i\cap B_j=\emptyset\text{ for }i\neq j
\end{align*}
\subsection{Conditional Prob.\ and Bayes}
\noindent\begin{align*}
    \mathbb{P}(B\mid A) & =\frac{\mathbb{P}(A\cap B)}{\mathbb{P}(A)}=\frac{\mathbb{P}(A\mid B)\mathbb{P}(B)}{\mathbb{P}(A)} \overset{\text{indep.}}{=} \mathbb{P}(B)
\end{align*}
If $\Omega=B_1\cup\cdots\cup B_k\quad$ with $B_i\cap B_j=\emptyset$ for $i\neq j$, then by the \textit{law of total probability}:
\noindent\begin{align*}
    \mathbb{P}(B_i\mid A) & =\frac{\mathbb{P}(A\cap B_i)}{\mathbb{P}(A)}=\frac{\mathbb{P}(A\mid B_i)\mathbb{P}(B_i)}{\mathbb{P}(A)}                              \\
                          & =\frac{\mathbb{P}(A\mid B_i)\mathbb{P}(B_i)}{\sum_{j=1}^k \underbrace{\mathbb{P}(A\mid B_j)\mathbb{P}(B_j)}_{\mathbb{P}(A\cap B_j)}}
\end{align*}

\textbf{Remark:} The conditional probability $\mathbb{P}(\cdot|B)$ can be viewed as a new probability measure over $\Omega=B$. Thus, the usual probability rules stated in Subsubsection\ \ref{sssec:rules_from_axioms} hold.

\section{Random Variables}
When using random variables, the event $A$ is a subset of $\mathbb{R}$, and its probability given by
\noindent\begin{align*}
    X : \Omega         & \rightarrow \mathbb{R}                                                                      \\
    \mathbb{P}(X\in A) & =\mathbb{P}(\underbrace{\{\omega\in\Omega:X(\omega)\in A\}}_{X^{-1}(A)\text{ := preimage}})
\end{align*}
Thus, events can be defined using intervals:
\noindent\begin{align*}
    \mathbb{P}(a\leq X\leq b) & =\mathbb{P}(X\in[a,b])
\end{align*}

\subsection{Discrete Random Variables}
\noindent\begin{equation*}
    X:\Omega\rightarrow W\subseteq\mathbb{R}
\end{equation*}
A random variable $X$ is said to be \textit{discrete} if its \textit{range} $W$ is \textbf{at most countable}.
\newpar{}
\ptitle{PMF}

The \textit{probability mass function (pmf)} of $X$ is given by
\noindent\begin{align*}
    p(x_{k})            & =\mathbb{P}(X=x_{k})                            \\
    \mathbb{P}(X\in A)  & =\sum_{k:x_k\in A}p(x_k)                        \\
    \sum_{k\geq1}p(x_k) & =1                       & \text{Normalization}
\end{align*}

\newpar{}
\ptitle{Distribution}

The \textit{distribution} of $X$ is the probabilty measure $\mathbb{Q}$ on $\mathbb{R}$ given by
\noindent\begin{equation*}
    \mathbb{Q}(A)=\mathbb{P}(X\in A)=\sum_{k:x_k\in A} \underbrace{p(x_k)}_{\text{pmf}}\quad\mathrm{for~}A\subseteq\mathbb{R}
\end{equation*}

\newpar{}
\ptitle{CDF}

The \textit{cumulative distribution function (cdf)} of $X$ is the function
\noindent\begin{align*}
    F:\mathbb{R}            & \rightarrow [0,1]                             \\
    F(x)=\mathbb P(X\leq x) & =\sum_{k:x_k\leq x}p(x_k),\quad x\in\mathbb R
\end{align*}

\subsubsection{Properties of CDFs}
\begin{itemize}
    \item F is non-decreasing (weakly increasing)
    \item $\lim_{x\rightarrow -\infty}F(x)=0$ and $\lim_{x\rightarrow \infty}F(x)=1$
    \item F is right-continous.\ i.e. $\lim_{y\downarrow x} F(y) =F(x)$
\end{itemize}

\subsubsection{General Rules for Discrete Random Variables}
\noindent\begin{align*}
    \mathbb{P}(X>x)       & =1-\mathbb{P}(X\leq x)=1-F(x)                                   \\
    \mathbb{P}(X\geq x)   & =\mathbb{P}(X>x)+\mathbb{P}(X=x)=1-F(x)+p(x)                    \\
    \mathbb{P}(a<X\leq b) & =\mathbb{P}(X\leq b)-\mathbb{P}(X\leq a)=F(b)-F(a)              \\
    p(x_k)                & \overset{\text{adjacent in } W}{=}\mathbb{P}(x_{k-1}<X\leq x_k) \\
                          & =F(x_k)-F(x_{k-1})
\end{align*}

\subsection{Independence of Random Variables}
Two random variables $X,Y_\Omega \rightarrow\mathbb{R}$ are \textbf{independent} if
\noindent\begin{align*}
    \mathbb{P}(X\in A,Y\in B)            & =                                                                                     \\
    \mathbb{P}(\{X\in A\}\cap\{Y\in B\}) & \overset{!}{=}\mathbb{P}(X\in A)\mathbb{P}(Y\in B)\quad\forall A,B\subseteq\mathbb{R}
\end{align*}

Several random variables $X_a, \dots X_n :\Omega\rightarrow\mathbb{R}$ are \textbf{independent} if
\noindent\begin{align*}
    \mathbb{P}(X_1\in A_1,\ldots,X_n\in A_n) = & \mathbb{P}(X_1\in A_1)\cdots\mathbb{P}(X_n\in A_n) \\
                                               & \forall A_1,\ldots,A_n\subseteq\mathbb{R}
\end{align*}

\subsubsection{Expectation}
The \textbf{expectation} of a discrete random variable $X$
\noindent\begin{equation*}
    \mu_{X}=\mathbb{E}[X]=\sum_{k\geq1}x_{k}p(x_{k})\in\mathbb{R}
\end{equation*}
can be interpreted as the
\begin{itemize}
    \item mean location of the distribution
    \item average under a large number of repetitions
    \item center of mass of the pmf.
\end{itemize}

\newpar{}
If $Y=g(X)$ is a \textbf{transformation} of the random variable $X$, its \textbf{expectation value} is given by
\noindent\begin{align*}
    \mathbb{E}[Y]    & =\mathbb{E}[g(X)]=\sum_{k\geq1}g(x_{k})p(x_{k})                    \\
    \mathbb{E}[g(X)] & \neq g(\mathbb{E}[X])                           & \text{generally}
\end{align*}

\subsubsection{Properties of the Expectation}
These properties also hold for non-discrete random variables if $\mathbb{E}[X]$ and $\mathbb{E}[Y]$ are well-defined.
\noindent\begin{align*}
    \textbf{Constants:}                   &  &  & a\in\mathbb{R},\mathbb{E}[a]=a\cdot p(a)=a                            \\
    \textbf{Linearity:}                   &  &  & a\mathbb{E}[X]=\mathbb{E}[aX],\;\;a\in \mathbb{R}                     \\
                                          &  &  & \mathbb{E}[X]+\mathbb{E}[Y]=\mathbb{E}[Z],\;\; Z=X+Y                  \\
    \textbf{Monotonicity:}                &  &  & \mathbb{E}[X]=\sum_{k\geq1}x_k\mathbb{P}(X=x_k)\geq0                  \\
    X(\omega)\geq Y(\omega)\forall\omega: &  &  & \operatorname{E}[X]-\operatorname{E}[Y]=\operatorname{E}[X-Y]\geq0    \\\\
    \text{Assuming}                       &  &  & \text{independece of } X,Y:                                           \\
    \textbf{Product:}                     &  &  & \mathbb{E}[X]\mathbb{E}[Y] = \mathbb{E}[XY] = \mathbb{E}[Z],\;\; Z=XY
\end{align*}

\subsubsection{Variance}
The \textbf{variance} of a discrete random variable
\noindent\begin{equation*}
    \mathrm{Var}(X) = \mathbb{E}[{(X-\mathbb{E}[X])}^2] = \sum_{k\geq1}{(x_k-\mu_X)}^2p(x_k)\geq0
\end{equation*}
can be interpreted as the
\begin{itemize}
    \item mean quadratic deviation from the mean
    \item measure of dispersion
\end{itemize}

\newpar{}
If $Y=g(X)$ is a \textbf{transformation} of the random variable $X$, its \textbf{variance} is given by
\noindent\begin{align*}
    \mathrm{Var}(g(X)) & =\mathbb{E}[{(g(X)-\mathbb{E}[g(X)])}^{2}]                                 \\
                       & =\sum_{k\geq1}{(g(x_{k})-\mathbb{E}[g(X)])}^{2}p(x_{k})                    \\
    \mathrm{Var}(g(X)) & \neq g(\mathrm{Var}(X))                                 & \text{generally}
\end{align*}

\subsubsection{Standard Deviation}
The \textbf{standard deviation} is the square root of the \textbf{variance}
\noindent\begin{equation*}
    \sigma_{X}=\sqrt{\mathrm{Var}(X)}
\end{equation*}
and has the same unit as $X$.

\subsubsection{Properites of the Standard Deviation and Variance}
The variance is \textbf{not linear}. (Assuming all expectations are well defined)
\noindent\begin{align*}
    \mathrm{Var}(X)      & = \mathbb{E}[X^2]-{\mathbb{E}[X]}^2                        \\
    \mathrm{Var}(X)\geq0 & \Rightarrow\mathbb{E}[X^{2}]\geq{\mathbb{E}[X]}^{2}        \\
    \mathrm{Var}(a)      & =\mathbb{E}[{(a-\mathbb{E}[a])}^2]=\mathbb{E}[{(a-a)}^2]=0 \\
    \mathrm{Var}(a+bX)   & = b^2\mathrm{Var}(X)
\end{align*}
\ptitle{Variance of Sums}

\noindent\begin{align*}
    \mathrm{Var}(a_1X_1 & +\cdots+a_n X_n)  =\sum_{i=1}^{n}a_{i}^{2}\mathrm{Var}(X_{i})+\ldots                  \\
                        & + \underbrace{2\sum_{i<j}a_{i}a_{j}\mathrm{Cov}(X_{i},X_{j})}_{0\text{ if indp.}}     \\
    \mathrm{Var}(X+Y)   & =\mathrm{Var}(X)+\mathrm{Var}(Y)+ \underbrace{2\mathrm{Cov}(X,Y)}_{0\text{ if indp.}} \\
    \mathrm{Var}(X-Y)   & =\mathrm{Var}(X)+\mathrm{Var}(Y)-\underbrace{2\mathrm{Cov}(X,Y)}_{0\text{ if indp.}}
\end{align*}

\ptitle{Standard Deviation}
\noindent\begin{equation*}
    \sigma_{a+bX}=\sqrt{\mathrm{Var}(a+bX)}=\sqrt{b^2\mathrm{Var}(X)}=|b|\sigma_X
\end{equation*}

\subsubsection{Convariance and Correlation}
The \textbf{covariance} between two (discrete) random variables $X,Y$
\noindent\begin{equation*}
    \operatorname{Cov}(X,Y)=\mathbb{E}[(X-\mathbb{E}[X])(Y-\mathbb{E}[Y])]
\end{equation*}
\begin{itemize}
    \item is (only) a measure of \textbf{linear dependence} between $X$ and $Y$
    \item can not tell wether $X$ causes $Y$, $Y$ causes $X$, or both are caused by a third variable $Z$
    \item if $\operatorname{Cov}(X,Y)$ and a certain $x$ are given wen can conclude on where we expect $y$ to be with respect to $\mathbb{E}[Y]$ (and vice versa)
\end{itemize}

\newpar{}
The \textbf{correlation} between two (discrete) random variables $X,Y$
\noindent\begin{equation*}
    \mathrm{Corr}(X,Y)=\rho_{XY}=\frac{\mathrm{Cov}(X,Y)}{\rho_{X}\rho_{Y}}
\end{equation*}
\begin{itemize}
    \item is a \textbf{normalized} version of the covariance:\newline$-1 \leq \rho_{XY} \leq 1$
    \item \textbf{Perfect positive correrlation}\newline $\rho_{XY}=1\Leftrightarrow Y=a+bX\mathrm{~for~}b>0$
    \item \textbf{Perfect negative correrlation}\newline $\rho_{XY}=-1\Leftrightarrow Y=a+bX\mathrm{~for~}b<0$
    \item if $Y=\exp(X)$, one has perfect dependence but $\mathrm{Corr}(X,Y)<1$
\end{itemize}

\subsubsection{Properites of the Covariance and Correlation}
(Assuming all expectations are well defined)
\noindent\begin{align*}
    \mathrm{Cov}(X,Y)        & = \mathbb{E}[XY]-\mathbb{E}[X]\mathbb{E}[Y]                              \\
    \mathrm{Cov}(X,X)        & = \mathrm{Var}(X)                                                        \\
    \operatorname{Cov}(X,Y)  & =\mathbb{E}[XY]-\mathbb{E}[X]\mathbb{E}[Y]=0 \Rightarrow X,Y\text{ idp.} \\
    \mathrm{Cov}(a+bX,c+dY)  & =bd\operatorname{Cov}(X,Y),\quad b,d\in\mathbb{R}                        \\
    \mathrm{Corr}(a+bX,c+dY) & ={\frac{bd}{|bd|}}\mathrm{Corr}(X,Y),\quad b,d\in\mathbb{R}
\end{align*}

\subsection{Discrete Distributions}

\subsubsection{Bernoulli}

\renewcommand{\arraystretch}{1.3}
\setlength{\oldtabcolsep}{\tabcolsep}\setlength\tabcolsep{3pt}
\begin{tabularx}{\linewidth}{@{}p{0.5\linewidth}p{0.49\linewidth}@{}}
    $W=\{0,1\}$                                     &
    \multirow{4}{*}{
        \begin{tikzpicture}
            % \pgfplotsset{ticks = none}
            \tiny
            \begin{axis}[
                    % xlabel={$x$},
                    ylabel={Probability},
                    legend style={at={(1,1)},anchor=north east},
                    legend style={font=\tiny},
                    ymin  = 0,
                    yticklabel=\empty,
                    ytick = \empty,
                    xtick={0,1},
                    height = 3cm,
                    width = 5cm,
                    grid style=dashed,
                    smooth,
                ]
                \addplot [
                    domain=0:1,
                    samples=2,
                    color=red,
                    ycomb,
                    line width = 2pt,
                ]
                {((1-0.2)^(x-1))*0.2};
            \end{axis}
        \end{tikzpicture}
    }                                                 \\
    $\mathbb{P}(X=0)=1-p,\newline\mathbb{P}(X=1)=p$ & \\
    $\mathbb{E}[X] = p$                             & \\
    $\mathrm{Var}(X) = p(1-p)$                      &
\end{tabularx}
\renewcommand{\arraystretch}{1}
\setlength\tabcolsep{\oldtabcolsep}



\subsubsection{Binomial}
$X$ = number of successes in $n$ independent Bernoulli experiments.

\renewcommand{\arraystretch}{1.3}
\setlength{\oldtabcolsep}{\tabcolsep}\setlength\tabcolsep{3pt}
\begin{tabularx}{\linewidth}{@{}p{0.5\linewidth}p{0.49\linewidth}@{}}
    $W=\{0,1,\ldots,n\}$                                                                                             &
    \multirow{4}{*}{
        \begin{tikzpicture}
            % \pgfplotsset{ticks = none}
            \tiny
            \begin{axis}[
                    xlabel={$x$},
                    ylabel={Probability},
                    legend style={at={(1,1)},anchor=north east},
                    legend style={font=\tiny},
                    ymin  = 0,
                    xtick={0,5,15},
                    xticklabels={0,$n_1p_1$,$n_2p_2$},
                    ytick = \empty,
                    yticklabel=\empty,
                    height = 3cm,
                    width = 5cm,
                    grid style=dashed,
                    bar width=1pt,
                ]
                \addplot [
                    domain=0:30,
                    samples=30,
                    color=red,
                    ybar,
                    draw opacity=1,
                    line width = 1pt,
                ]
                {factorial(150)/(factorial(x)*factorial(150-x))*(0.1^x)*((0.9)^(150-x))};
                \addlegendentry{$n=150, p=0.1$}

                \addplot [
                    domain=0:30,
                    samples=30,
                    color=blue,
                    ybar,
                    draw opacity=0.5,
                    line width = 1pt,
                ]
                {factorial(50)/(factorial(x)*factorial(50-x))*0.1^x*(0.9)^(50-x)};
                \addlegendentry{$n=50,p=0.1$}
            \end{axis}
        \end{tikzpicture}
    }                                                                                                                  \\
    $\mathbb{P}(X=k)={n\choose k}p^k{(1-p)}^{n-k}\newline k\in {0,1,\ldots, n},\; {n\choose k}=\frac{n!}{k!(n-k)!} $ & \\
    $\mathbb{E}[X] = np$                                                                                             & \\
    $\mathrm{Var}(X) = np(1-p)$                                                                                      &
\end{tabularx}
\renewcommand{\arraystretch}{1}
\setlength\tabcolsep{\oldtabcolsep}

\subsubsection{Geometric}
$X$ = number of independent Bernoulli trials until first success.

\renewcommand{\arraystretch}{1.3}
\setlength{\oldtabcolsep}{\tabcolsep}\setlength\tabcolsep{3pt}
\begin{tabularx}{\linewidth}{@{}p{0.5\linewidth}p{0.49\linewidth}@{}}
    $W=\{1,2,\ldots\}$                                      &
    \multirow{4}{*}{
        \begin{tikzpicture}
            % \pgfplotsset{ticks = none}
            \tiny
            \begin{axis}[
                    xlabel={$x$},
                    ylabel={Probability},
                    legend style={at={(1,1)},anchor=north east},
                    legend style={font=\tiny},
                    ymin  = 0,
                    ytick = \empty,
                    yticklabel=\empty,
                    height = 3cm,
                    width = 5cm,
                    grid style=dashed,
                    bar width=1pt,
                ]
                \addplot [
                    domain=1:15,
                    samples=15,
                    color=red,
                    ybar,
                    draw opacity=1,
                    line width = 2pt,
                ]
                {((1-0.2)^(x-1))*0.2};
                \addlegendentry{\(p=0.2\)}

                \addplot [
                    domain=1:15,
                    samples=15,
                    color=blue,
                    ybar,
                    draw opacity=0.5,
                    line width = 2pt,
                ]
                {((1-0.5)^(x-1))*0.5};
                \addlegendentry{\(p=0.5\)}
            \end{axis}
        \end{tikzpicture}
    }                                                         \\
    $\mathbb{P}(X=k)={(1-p)}^{k-1}p\newline k\in\mathbb{N}$ & \\
    $\mathbb{E}[X] = \frac{1}{p}$                           & \\
    $\mathrm{Var}(X) = \frac{1-p}{p^2}$                     &
\end{tabularx}
\renewcommand{\arraystretch}{1}
\setlength\tabcolsep{\oldtabcolsep}


\subsubsection{Poission}

\renewcommand{\arraystretch}{1.3}
\setlength{\oldtabcolsep}{\tabcolsep}\setlength\tabcolsep{3pt}
\begin{tabularx}{\linewidth}{@{}p{0.5\linewidth}p{0.49\linewidth}@{}}
    $W=\{0,1,2,\ldots\}$                                    &
    \multirow{4}{*}{
        \begin{tikzpicture}
            \tiny
            \begin{axis}[
                    xlabel={$x$},
                    ylabel={Probability},
                    legend style={at={(1,1)},anchor=north east},
                    legend style={font=\tiny},
                    xtick={0,3,9},
                    xticklabels={0,$\lambda_1$,$\lambda_2$},
                    ymin  = 0,
                    ytick = \empty,
                    yticklabel=\empty,
                    height = 3cm,
                    width = 5cm,
                    grid style=dashed,
                    bar width=1pt,
                ]
                \addplot [
                    domain=0:25,
                    samples=25,
                    color=red,
                    ybar,
                    draw opacity=1,
                    line width = 2pt,
                ]
                {exp(-3)*(3^x)/(factorial(x))};
                \addlegendentry{$\lambda = 3$}

                \addplot [
                    domain=0:25,
                    samples=25,
                    color=blue,
                    ybar,
                    draw opacity=0.5,
                    line width = 2pt,
                ]
                {exp(-8)*(8^x)/(factorial(x))};
                \addlegendentry{$\lambda = 8$}
            \end{axis}
        \end{tikzpicture}
    }                                                         \\
    $\mathbb{P}(X=k)=e^{-\lambda}\frac{\lambda^k}{k!}\newline k\in\mathbb{N}_0$ & \\
    $\mathbb{E}[X] = \lambda$                           & \\
    $\mathrm{Var}(X) = \lambda^2$                     &
\end{tabularx}
\renewcommand{\arraystretch}{1}
\setlength\tabcolsep{\oldtabcolsep}
\textbf{Remark:} For large $n$ and small $p$, $\mathrm{Bin}(n,p)\simeq \mathrm{Pois}(\lambda)$.

For independent $X \sim \mathrm{Pois}(\lambda_1)$ and $Y \sim \mathrm{Pois}(\lambda_2)$ one has $X+Y \sim \mathrm{Pois}(\lambda_1 +\lambda_2)$

\subsection{Continous Distributions}

\subsubsection{Densities}

\subsubsection{Quantiles}

\subsubsection{Simulation of Random Variables}

\section{Combinations}
\renewcommand{\arraystretch}{1.3}
\setlength\tabcolsep{6pt} % default value: 6pt
\begin{tabularx}{\linewidth}{@{}p{0.2\linewidth}ll@{}}
                                    & Repeated                                                  & Not Repeated                                           \\
    \cmidrule{2-3}
    Variation\newline (order)       & $n^k$                                                     & ${n\choose k} k!= \frac{n!}{(n-k)!}$                   \\
    Combinations\newline (no order) & ${n+k-1 \choose k}=\frac{(n+k-1)!}{(n-1)k!}$              & ${n\choose k} = {n \choose n-k} = \frac{n!}{(n-k)!k!}$ \\
    Permutations                    & $\frac{n!}{\Pi_i k_i !}= {n\choose k_1,k_2, \ldots, k_i}$ & n!                                                     \\
\end{tabularx}
\renewcommand{\arraystretch}{1}
\setlength\tabcolsep{6pt} % default value: 6pt
where $n=|\Omega|$, $k$ is the number of draws and $k!$ the number of possible arangement inside one group.