% rulecolor=\color{black},                % Frame color is black
% frame=single,                           % Single frame around the code
% numbers=left,                           % Line numbers on the left
% stepnumber=1,                           % Show every line number
% numberstyle=\tiny\color{gray},          % Line numbers in tiny font and gray color
% basicstyle=\scriptsize\ttfamily,        % Use smaller font size to reduce width
\definecolor{darkgreen}{RGB}{0,150,0}
\lstset{ 
    language=Python,
    basicstyle=\footnotesize\ttfamily,      % Use smaller font size to reduce width
    keywordstyle=\color{blue}\bfseries,     % Keywords in blue and bold
    stringstyle=\color{orange},             % Strings in orange
    commentstyle=\color{darkgreen}\itshape, % Comments in gray and italic
    identifierstyle=\color{black},          % Identifiers in black
    showspaces=false,                       % Do not show spaces with underscores
    showstringspaces=false,                 % Do not underline spaces in strings
    showtabs=false,                         % Do not show tabs with underscores
    tabsize=2,                              % Tab size equivalent to 2 spaces
    breaklines=true,                        % Automatic line breaking
    breakatwhitespace=true,                 % Break lines at whitespace if possible
    numbersep=4pt,                          % Reduce space between line numbers and code
    framexleftmargin=3pt,                   % Smaller left margin
    framexrightmargin=3pt,                  % Smaller right margin
    framextopmargin=3pt,                    % Smaller top margin
    framexbottommargin=3pt,                 % Smaller bottom margin
    captionpos=b,                           % Caption position is at the bottom
}

\section{Appendix}
\subsection{Matrix Algebra}
For $\mathbf{w}\in \mathbb{R}^d, \mathbf{A}\in \mathbb{R}^{d\times d}$ and $ f: \mathbb{R}^d\to \mathbb{R}$
\noindent\begin{align*}
    \frac{\partial f}{\partial \mathbf{w}}                                                                                             & =
    \begin{pmatrix}\frac{\partial f}{\partial \mathbf{w}_1} & \cdots & \frac{\partial f}{\partial \mathbf{w}_d}
    \end{pmatrix}^{\mathsf{T}} &                                                                                                                 & \in \mathbb{R}^d                                                                                                                                                \\[1em]
    \frac{\partial \mathbf{w}^{\mathsf{T}}\mathbf{w}}{\partial \mathbf{w}}                                                             & =\frac{\partial\|\mathbf{w}\|^2}{\partial \mathbf{w}}=2\mathbf{w}                                               &                  & \in \mathbb{R}^d                     \\
    \frac{\partial \mathbf{A}\mathbf{w}}{\partial \mathbf{w}}                                                                          & ={\mathbf{A}}^{\mathsf{T}}                                                                                      &                  & \mathbf{A}\in \mathbb{R}^{d\times d} \\
    \frac{\partial \mathbf{w}^{\mathsf{T}}\mathbf{A}}{\partial \mathbf{w}}                                                             & = \mathbf{A}                                                                                                    &                  & \mathbf{A}\in \mathbb{R}^{d\times d} \\
    \frac{\partial \mathbf{w}^{\mathsf{T}}\mathbf{Aw}}{\partial \mathbf{w}}                                                            & = \left(\mathbf{A}+\mathbf{A}^{\mathsf{T}}\right)\mathbf{w} \overset{\mathbf{A}\ \mathrm{sym.}}{=} 2\mathbf{Aw} &                  & \in \mathbb{R}^{d\times d}
\end{align*}

\subsection{Coding}
% {\scriptsize
%{\small
{\footnotesize
\begin{lstlisting}[language=Python]
    # sklearn basic stuff #########
    from sklearn.decomposition import PCA
    from sklearn.preprocessing import RobustScaler
    from sklearn.cluster import KMeans
    pca = PCA(); pca_fitted = pca.fit(X_raw)    # PCA
    var_cms = pca_fitted.explained_variance_ratio_.cumsum()
    pca = PCA(n_components=3).fit(X_raw)
    X_pca = pca.transform(X_raw)
    scaler = RobustScaler().fit(X_pca)          # scaler
    X_pca_sc = scaler.transform(X_pca)
    kmeans = KMeans(n_clusters=8, n_init=20)    # k-means
    y_pred = kmeans.fit_predict(X_pca_sc)
    pca.inverse_transform(scaler.inverse_transform(
        kmeans.cluster_centers_))
    
    # sklearn simple models #########
    from sklearn.tree import DecisionTreeClassifier
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.metrics import balanced_accuracy_score
    from sklearn.model_selection import train_test_split
    X_train, X_test, y_train, y_test = train_test_split(
        images, distances, test_size=0.2, random_state=0)
    y_dt_raw = DecisionTreeClassifier(random_state=42)  
    y_dt_raw.fit(X_train, y_train)
    y_pred_dt_raw = y_dt_raw.predict(X_test)
    bar_dt_raw = balanced_accuracy_score(
        y_test, y_pred_dt_raw)
    rf = RandomForestClassifier(random_state = 11)      
    rf.fit(X_train, y_train)
    y_rf = rf.predict(X_test)
    bar_rf_raw = balanced_accuracy_score(y_test, y_rf)
    
    # sklearn pipelines #########
    from sklearn.kernel_ridge import KernelRidge
    from sklearn.svm import SVR
    from sklearn.model_selection import train_test_split, GridSearchCV
    from sklearn.pipeline import Pipeline
    scaler = MinMaxScaler(); pca = PCA()
    model = SVR(kernel= 'rbf')
    pipe = Pipeline([('scaler', scaler),
                        ('pca', pca),
                        ('model', model)], verbose=True)
    pipe.fit(X_train, y_train)
    param_grid_svr = {
        "pca__n_components": np.linspace(1, 
        200, 200, dtype=int),
        "model__C": np.logspace(-3, 1, 10),
        "model__gamma": np.logspace(-3, 1, 10),
    }
    grid = GridSearchCV(pipe, param_grid_svr, cv = 3, 
                        scoring = "neg_mean_absolute_error", 
                        verbose = 2)
    grid.fit(X_train, y_train) 
    print("[INFO]:n_features:", grid.n_features_in_)
    print("[INFO]:best_est:", grid.best_estimator_)
    model_grid = grid.best_estimator_
    y_pred = model_grid.predict(X_val)
    
    # pytorch #########
    from torch.utils.data import Dataset, DataLoader
    (img0, gt0) = train_dataset[11]
    transform = transforms.Compose(
        [transforms.ToTensor(), 
        transforms.Normalize((0.5,), (0.5,))])
    class CNNModel(nn.Module):
        def __init__(self, dropout_probability=0.):
            super(CNNModel, self).__init__()
            self.conv1 = nn.Conv2d(3, 64, 
            kernel_size=3, padding=1) #...
            self.pool = nn.MaxPool2d(2, 2)
            self.drop = nn.Dropout(p=dropout_probability)       
            self.fc1 = nn.Linear(256 * 8 * 8, 128) #...
        def forward(self, x):
            x = self.pool(F.relu(self.conv1(x)))
            x = self.drop(x)
            x = F.relu(self.conv3(x)); x = self.drop(x)                
            x = x.view(-1,256 * 8 * 8)
            x = F.relu(self.fc1(x))
            x = self.fc2(x)     # no relu! (internal softmax)    
            return x
    device = torch.device(
        "cuda" if torch.cuda.is_available() else "cpu")
    model = CNNModel()
    model.to(device)
    loss_fnc = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001)  
    for epoch in num_epochs:
        model.train()
        for imgs, targets in tqdm(train_dataloader):
            imgs, targets = imgs.to(device), targets.to(device)
            optim.zero_grad()
            output = model(imgs)
            loss = loss_fnc(output, targets)
            bloss.backward()
            optimizer.step()
        if epoch%VAL_FREQ == 0:
            model.eval()
            with torch.no_grad():
                for val_img, val_gt in val_dataloader:
                    val_img = val_img.to(device)    #only one
                    output = model(val_img)
                    val_crit = func(output, val_gt)      
\end{lstlisting}
}

{\small
\renewcommand{\arraystretch}{1.3}
\setlength{\oldtabcolsep}{\tabcolsep}\setlength\tabcolsep{6pt}

\begin{tabularx}{\linewidth}{@{}lll@{}}
                 & regular                    & empirical                    \\
    std.\ dev.\  & \fncode{np.std(a)}         & \fncode{np.std(a, ddof=1)}   \\
    mean         & \fncode{np.mean(a)}        &                              \\
    covariance   & \fncode{np.cov(a,b)}       & \fncode{np.cov(a,b, ddof=1)} \\
    correlation  & \fncode{np.corrcoeff(a,b)} &
\end{tabularx}

\renewcommand{\arraystretch}{1}
\setlength\tabcolsep{\oldtabcolsep}
}